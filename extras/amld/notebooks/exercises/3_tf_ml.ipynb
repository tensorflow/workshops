{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["# Machine Learning using Tensorflow\n", "\n", "In this notebook we will solve two simple exercises using low-level\n", "Tensorflow.\n", "\n", "Table of Contents:\n", "\n", "- [ 1 Iterative division](#1-Iterative-division)\n", "- [ 2 Neural function approximator](#2-Neural-function-approximator)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import time\n", "import tensorflow as tf\n", "import numpy as np\n", "from matplotlib import pyplot\n", "# Always make sure you are using running the expected version.\n", "# There are considerable differences between versions...\n", "tf.__version__"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%run -i _derived/2_visualize_graph.py\n", "# (Load code helper code cell -- make sure to have executed notebook \"2_tf_basics\" first.)"], "outputs": [], "metadata": {}}, {"source": ["# 1 Iterative division\n", "\n", "Using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n", "to iteratively solve a division:\n", "We first define our graph `a_times_b = a * b` and then we provide both\n", "`a` and `a_times_b`, and set an initial guess of `b` to zero. We then\n", "iteratively improve our guess for `b` by minimizing the square of the\n", "difference of `a_times_b` (=target) and `a * b`.\n", "\n", "Note that we do not need to specify any gradients -- Tensorflow's\n", "`GradientDescentOptimizer` can simply look at our \"forward graph\" and\n", "then generate a \"backward graph\" starting from our loss that consists\n", "of the gradients!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Use new graph for this section to keep things tidy.\n", "graph = tf.Graph()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Define our calculation + loss.\n", "\n", "with graph.as_default():\n", "    a = tf.placeholder(shape=(), dtype=tf.float32, name='a')\n", "    b = tf.Variable(0.0, name='b')\n", "    a_times_b = tf.placeholder(shape=(), dtype=tf.float32, name='a_times_b')\n", "    # Use overloaded Python operators for convenience.\n", "    # (this will translate to tf.mul(), tf.sub(), and tf.pow())\n", "    loss = (a_times_b - a * b) ** 2\n", "    # Let's use the identity Op to add a name to the \"loss\" tensor.\n", "    loss = tf.identity(loss, name='loss')\n", "\n", "    show_graph(tf.get_default_graph())"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Magic happens : add optimizer.\n", "\n", "with graph.as_default():\n", "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n", "    # We can now minimize the loss by calling this train_op (many times),\n", "    # which will change the value of \"b\" upon every invocation a bit in such\n", "    # a way that the loss becomes smaller.\n", "    train_op = optimizer.minimize(loss)\n", "\n", "    show_graph(tf.get_default_graph())"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Do the computation.\n", "\n", "with tf.Session(graph=graph) as sess:\n", "    # Variables must be initialized before first use.\n", "    tf.global_variables_initializer().run()\n", "    # Feed the same result at every step.\n", "    feed_dict = {a: 2, a_times_b: 42}\n", "\n", "    print 'b=', sess.run([b])\n", "    for i in range(10):\n", "        # Update b by calling train_op.\n", "        sess.run([train_op], feed_dict=feed_dict)\n", "        # Print our updated guess for b.\n", "        print 'b=', sess.run([b])"], "outputs": [], "metadata": {}}, {"source": ["# 2 Neural function approximator"], "cell_type": "markdown", "metadata": {}}, {"source": ["Neural networks can be seen as generic function approximators\n", "`y = f(x)` -- in this section we will use a network to approximate\n", "a simple mathematical function with a scalar input.\n", "\n", "The same approach can be used to train arbitrary function (for example\n", "a function where `x` is a dense vector of a megapixel image, and `y` is\n", "a probability distribution over thousands of different object classes).\n", "\n", "A crucial part when designing such a network is a good choice of\n", "\"hyperparameters\" that define *how* the network should be created and\n", "trained."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Helper class for function approximation. Defining all the code\n", "# in a single class makes it easier to instantiate different\n", "# \"versions\" of the approximator (to approximate different functions\n", "# with different configurations).\n", "\n", "class FunctionApproximator(object):\n", "    \"\"\"A class for function approximation using Tensorflow.\n", "\n", "    This class implements both building the graph and updating the\n", "    variables to better approximate the target function, so that it\n", "    can be instantiated with different functions/configurations.\n", "    \"\"\"\n", "\n", "    def __init__(self, f, hidden_layers):\n", "        \"\"\"Initializes a new function approximator.\n", "        \n", "        Args:\n", "          f: Scalar Python functional that should be approximated.\n", "            Signature: f(x:Number) -> Number\n", "          hidden_layers: Iterable specifying number of units per layer.\n", "        \"\"\"\n", "        self.f = f\n", "        self.hidden_layers = hidden_layers\n", "        # Initialize new graph that only contains Ops needed for\n", "        # function approximation.\n", "        self.graph = tf.Graph()\n", "        with self.graph.as_default():\n", "            self._build()\n", "        self.init()\n", "\n", "    def init(self):\n", "        \"\"\"Initializes weights.\"\"\"\n", "        self.sess = tf.Session(graph=self.graph)\n", "        self.init_op.run(session=self.sess)\n", "\n", "    def _build(self):\n", "        # The values for \"x\" and \"y\" are provided during training.\n", "        # Note the dynamic first dimension (=number of samples in batch).\n", "        self.x = tf.placeholder(shape=(None,), dtype=tf.float32)\n", "        self.y = tf.placeholder(shape=(None,), dtype=tf.float32)\n", "        # Reshape e.g. [1, 2, 3] to [[1], [2], [3]] because this latter\n", "        # shape is required by tf.layers() below.\n", "        x = tf.reshape(self.x, (-1, 1))\n", "        for units in self.hidden_layers:\n", "            # We use tf.layers() helper function to create \"fully connected\n", "            # layers\" where every neuron is connected to every neuron in the\n", "            # previous layer. This helper function takes care of defining\n", "            # variables and initializing them.\n", "            x = tf.layers.dense(inputs=x, units=units, activation=tf.nn.relu)\n", "        # Our predicted \"y\".\n", "        self.y_ = tf.layers.dense(inputs=x, units=1, activation=None)\n", "        # Reshape e.g. [[1], [2], [3]] (by tf.layers()) to [1, 2, 3].\n", "        self.y_ = tf.reshape(self.y_, (-1,))\n", "\n", "        # Compute loss.\n", "        self.loss = tf.reduce_mean((self.y - self.y_)**2)\n", "        # Add Ops for minimizing loss.\n", "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n", "        self.train_op = optimizer.minimize(self.loss)\n", "        self.init_op = tf.global_variables_initializer()\n", "\n", "    def fit(self, x_min, x_max, steps, batch_size, show_progress=False):\n", "        \"\"\"Fits the weights to better approximate the function.\n", "\n", "        This function will generate random samples \"x\" within [x_min, x_max]\n", "        and update the network's weight to better approximate y=f(x).\n", "\n", "        Args:\n", "          x_min: Lower bound for generation of \"x\".\n", "          x_max: Upper bound for generation of \"x\".\n", "          steps: Number of training steps.\n", "          batch_size: Number of samples to train network with in every step.\n", "        \"\"\"\n", "        t0 = time.time()\n", "\n", "        losses = []\n", "        for step in range(steps):\n", "            # Generate samples.\n", "            x_data = np.random.uniform(low=x_min, high=x_max, size=batch_size)\n", "            y_data = self.f(x_data)\n", "            feed_dict = {\n", "                self.x: x_data,\n", "                self.y: y_data\n", "            }\n", "            # Update the weights and get the loss.\n", "            loss_data, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n", "            losses.append(loss_data)\n", "            if show_progress and step % (steps // 10) == 0:\n", "                print 'step=%6d loss=%f' % (step, loss_data)\n", "\n", "        dt = time.time() - t0\n", "        return losses, dt\n", "\n", "    def predict(self, x_data):\n", "        \"\"\"Computes approximated function values.\"\"\"\n", "        return self.sess.run([self.y_], {self.x: x_data})[0]"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# \"hidden_layers\" specifies the number of neurons per layer.\n", "# The network will be trained with batch_size x \"steps\" random datapoints.\n", "\n", "# YOUR ACTION REQUIRED:\n", "# Try to achieve a good function approximation by finding good parameters...\n", "hidden_layers, steps = [5, 5, 5, 5, 5], 1000\n", "\n", "tf_sin = FunctionApproximator(f=np.sin, hidden_layers=hidden_layers)\n", "\n", "# Train the network.\n", "losses, dt = tf_sin.fit(x_min=-3., x_max=3., steps=steps, batch_size=1000,\n", "                        show_progress=True)\n", "print '%.2f seconds' % dt\n", "\n", "# Visualize some predictions\n", "x_data = np.random.uniform(low=-3., high=3., size=100)\n", "x_data.sort()\n", "y_data = np.sin(x_data)\n", "y_predictions = tf_sin.predict(x_data)\n", "\n", "pyplot.scatter(x_data, y_data)\n", "pyplot.plot(x_data, y_predictions, 'r-')"], "outputs": [], "metadata": {}}, {"source": ["# A References\n", "\n", "Unfortunately, we don't have time to talk about more ML in this\n", "workshop. Two good online references:\n", "\n", "- http://deeplearningbook.org (by Ian Goodfellow et al) \u2013\u00a0more theoretical\n", "- http://neuralnetworksanddeeplearning.com/ (by Michael Nielson) \u2013\u00a0more hands-on"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.12", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}