{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["# Building an Estimator\n", "\n", "Once we have our data in the \"right\" format (i.e. files containing\n", "`tf.train.Example` protocol buffers), Tensorflow makes the actual\n", "ML part straightforward.\n", "\n", "In this notebook we will first set up the computational graph that\n", "reads the input data and transforms it into tensors, and then use\n", "Tensorflow's \"canned\" estimator to learn from there.\n", "\n", "This notebook contains a whole series of exciting bonus sections\n", "that you certainly won't have time to solve during the workshop,\n", "but you might want to come back to these after the workshop to create\n", "more sophisticated models that solve the task better.\n", "\n", "Table of Contents:\n", "\n", "- [ 1 Reading the data](#1-Reading-the-data)\n", "  - [ 1.1 Reading the data using pure Python](#1.1-Reading-the-data-using-pure-Python)\n", "  - [ 1.2 Reading the data using Tensorflow](#1.2-Reading-the-data-using-Tensorflow)\n", "- [ 2 Canned estimators](#2-Canned-estimators)\n", "- [ 3 Custom convolutional classifier \u2013\u00a0bonus!](#3-Custom-convolutional-classifier-\u2013-bonus!)\n", "- [ 4 Keras \u2013\u00a0bonus!](#4-Keras-\u2013-bonus!)\n", "- [ 5 Recurrent neural network \u2013\u00a0bonus!](#5-Recurrent-neural-network-\u2013-bonus!)\n", "- [A References](#A-References)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# import packages used in this notebook\n", "import itertools\n", "import numpy as np\n", "import tensorflow as tf\n", "from matplotlib import pyplot\n", "\n", "%matplotlib inline\n", "# what version of Tensorflow are we running ? should be 1.4.X\n", "print tf.__version__"], "outputs": [], "metadata": {}}, {"source": ["# 1 Reading the data"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Load data generated in 1_qd_data:\n", "data_path = '../data/dataset_img'\n", "# Show files in dataset.\n", "!ls -l -h $data_path"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# It's customary to identify the label by an integer number. These\n", "# numbers map to the different classes as follows:\n", "classes = open('%s/labels.txt' % data_path).read().splitlines()\n", "print '%d label classes:\\n' % len(classes)\n", "for i, label in enumerate(classes):\n", "    print '%d -> %s' % (i, label)"], "outputs": [], "metadata": {}}, {"source": ["## 1.1 Reading the data using pure Python"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Step 1 : Read a single record from the first file.\n", "train_files = tf.gfile.Glob('%s/train-*' % data_path)\n", "record = tf.python_io.tf_record_iterator(train_files[0]).next()\n", "# Step 2 : Parse record into tf.train.Example proto.\n", "example = tf.train.Example.FromString(record)\n", "# We need the image \"img_64\" and the \"label\". Both are of type\n", "# int64_list: the label is stored by its index and the image is\n", "# a 64x64 list of integers that are the pixel values.\n", "for name, feature in example.features.feature.items():\n", "    print '%20s (%s)' % (name, feature.WhichOneof('kind'))"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Step 3 : Read features.\n", "img_64 = example.features.feature['img_64'].int64_list.value\n", "img_64 = np.array(img_64).reshape((64, 64)) / 255.\n", "label = example.features.feature['label'].int64_list.value[0]"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Visualize drawing.\n", "\n", "def show_img(img_64, title, ax=None):\n", "    (ax if ax else pyplot).matshow(img_64, cmap='gray')\n", "    ax = ax if ax else pyplot.gca()\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(title)\n", "\n", "show_img(img_64, classes[label])"], "outputs": [], "metadata": {}}, {"source": ["## 1.2 Reading the data using Tensorflow"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_input_fn_img.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Read input data from sharded files using 1.4 API.\n", "# This is copied and slightly simplified from tf_snippets.ipynb\n", "\n", "# Note that this cell simply defines the functions but does not yet call\n", "# them. The functions are executed in the next cell within a tf.Graph()\n", "# context.\n", "\n", "# This dictionary specifies what \"features\" we want to extract from the\n", "# tf.train.Example protos (i.e. what they look like on disk). We only\n", "# need the image data \"img_64\" and the \"label\". Both features are tensors\n", "# with a fixed length.\n", "# You need to specify the correct \"shape\" and \"dtype\" parameters for\n", "# these features...\n", "feature_spec = {\n", "    # Single label per example => shape=[1] (we could also use shape=() and\n", "    # then do a transformation in the input_fn).\n", "    'label': tf.FixedLenFeature(shape=[1], dtype=tf.int64),\n", "    # The bytes_list data is parsed into tf.string.\n", "    'img_64': tf.FixedLenFeature(shape=[64, 64], dtype=tf.int64),\n", "}\n", "\n", "def parse_example(serialized_example):\n", "    # Convert string to tf.train.Example and then extract features/label.\n", "    features = tf.parse_single_example(serialized_example, feature_spec)\n", "    # Important step: remove \"label\" from features!\n", "    # Otherwise our classifier would simply learn to predict\n", "    # label=features['label']...\n", "    label = features.pop('label')\n", "    # Convert int64 [0..255] to float [0..1]\n", "    features['img_64'] = tf.cast(features['img_64'], tf.float32) / 255.\n", "    return features, label\n", "\n", "# Common Tensorflow pattern : wrap function to specify parameters.\n", "# The estimator interface expects a \"input_fn\" as a parameter, and not the\n", "# tensors (features, labels) so it can re-create the tensors in different\n", "# graphs later on.\n", "def make_input_fn(files_pattern, batch_size=100):\n", "    def input_fn():\n", "        # Signature input_fn: () -> features=Dict[str, Tensor], labels=Tensor\n", "        ds = tf.data.TFRecordDataset(tf.gfile.Glob(files_pattern))\n", "        ds = ds.map(parse_example).batch(batch_size)\n", "        ds = ds.shuffle(buffer_size=5*batch_size).repeat()\n", "        features, labels = ds.make_one_shot_iterator().get_next()\n", "        return features, labels\n", "    return input_fn"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Define a graph, load a batch of examples and display one of them.\n", "\n", "%run -i _derived/4_input_fn_img.py\n", "\n", "# First, we create the input_fn -- this defines which files to read from\n", "# and the batch size, and can later be passed around.\n", "batch_size = 5\n", "input_fn = make_input_fn(files_pattern='%s/train-*' % data_path,\n", "                         batch_size=batch_size)\n", "with tf.Graph().as_default():\n", "    # After registering a default graph, we call the previously defined\n", "    # input_fn.\n", "    # This yields the dictionary of feature tensors and the label tensor.\n", "    features, labels = input_fn()\n", "    with tf.train.MonitoredSession() as sess:\n", "        # Note: We create a tf.train.MonitoredSession instead of a normal\n", "        # tf.Session because Tensorflow is using pipelines under the hood\n", "        # to read data in a streaming fashion from disk into our computational\n", "        # Graph. If we used a plain old tf.Session(), then we would have to\n", "        # take care of initializing the threads for the pipelines ourselves...\n", "        img_64_, labels_ = sess.run([features['img_64'], labels])\n", "        # Note that first dimension is the batch dimension.\n", "        for i in range(batch_size):\n", "            ax = pyplot.subplot(1, batch_size, i+1)\n", "            show_img(img_64_[i], classes[labels_[i][0]], ax)\n", "\n", "# Also note how content changes upon every re-execution of this cell.\n", "# That is because of the randomization in the input_fn."], "outputs": [], "metadata": {}}, {"source": ["# 2 Canned estimators\n", "\n", "Once we have our `input_fn`, it's a piece of cake to do some\n", "machine learning on top of it using Tensorflow's \"canned\n", "estimators\". Let's start with a simple\n", "[LinearClassifier](https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/LinearClassifier).\n", "\n", "A LinearClassifier implements simple **logistic regression** that is basically\n", "a neural network where every input neuron (pixel image) is directly connected\n", "to all output neurons (probabilities for different classes). Every output neuron\n", "effectively remembers a \"mask\" in the input pixel space, an approach that\n", "quickly learns to identify drawings that have similar pixel activations, but\n", "fails to learn anything more complicated (e.g. an elephant's trunk could\n", "activate the giraffe's \"neck mask\" if it happened to be in the same pixel\n", "region)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# \"Feature columns\" tell our canned estimator how to make use of the\n", "# data returned by input_fn(). In our case we have a single feature that\n", "# is of numerical (defaults to tf.float32) type with shape=(64, 64).\n", "feature_columns = [\n", "    # Add a feature column of numeric type and correct shape for the \"img_64\" feature.\n", "    tf.feature_column.numeric_column('img_64', shape=(64, 64))\n", "]\n", "\n", "# Note that feature columns can also be used to generate the \"feature_spec\"\n", "# used to parsing the tf.train.Example protocol buffers, and that there are\n", "# other feature column types that can do complicated data transformation\n", "# (such as looking up strings and creating embeddings) -- but that's a story\n", "# for another workshop ;-)\n", "\n", "# (we also need to specify number of output classes)\n", "linear_estimator  = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n", "                                                  n_classes=len(classes))"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Connect it to the input_fn with the training data and train the classifier.\n", "\n", "# YOUR ACTION REQUIRED:\n", "# You have to specify what data to use for training (which input_fn), and how\n", "# many steps you want to train for. The number of examples used for training is\n", "# = steps * batch_size.\n", "input_fn = make_input_fn(...)\n", "\n", "# YOUR ACTION REQUIRED:\n", "# Try different number of training steps and see how this influences training time\n", "# (and accuracy in cell below). Note that you have to re-run the previous cell\n", "# to \"reset\" the classifier (otherwise the estimator will simply *continue* the\n", "# training with an additional number of steps).\n", "steps =\n", "linear_estimator.train(input_fn=input_fn, steps=steps)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Evaluate the fitted classifier.\n", "# YOUR ACTION REQUIRED:\n", "# Find the correct values for below variables. Note that you should *not*\n", "# evaluate with the same data used for training.\n", "input_fn = \n", "steps =\n", "linear_estimator.evaluate(input_fn=input_fn, steps=steps)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# The simple LinearClassifier does not perform too well... Let's examine\n", "# some classifications so we can see which cases were failing.\n", "\n", "# Because estimator.predict() only returns class probabilities and not the\n", "# input data, we parse the features in Python (which gives us a handle on\n", "# the input data) and then create an input_fn using numpy_input_fn().\n", "\n", "def get_label_img(files_pattern, n):\n", "    \"\"\"Reads batch of \"img_64\" and \"label\" features.\n", "\n", "    Args:\n", "      files_pattern: Pattern matching files containing tf.train.Example.\n", "      n: Number of examples to parse.\n", "\n", "    Returns:\n", "      (imgs, labels) with shapes (n, 64, 64) and (n, 1). The image pixel values\n", "      fall in the range 0..1.\n", "    \"\"\"\n", "    imgs = np.zeros((n, 64, 64), dtype=np.float32)\n", "    labels = np.zeros((n, 1), dtype=np.int64)\n", "    i = 0\n", "    for filename in  tf.gfile.Glob(files_pattern):\n", "        for record in tf.python_io.tf_record_iterator(filename):\n", "            example = tf.train.Example.FromString(record)\n", "            labels[i, 0] = example.features.feature['label'].int64_list.value[0]\n", "            img_64 = example.features.feature['img_64'].int64_list.value\n", "            imgs[i, :, :] = np.array(img_64).reshape((64, 64)) / 255.\n", "            i += 1\n", "            if i >= n:\n", "                return imgs, labels\n", "\n", "def show_predictions(estimator, rows=4, cols=4, predict_keys=None):\n", "    \"\"\"Shows predictions + labels for a couple of examples.\"\"\"\n", "    # Read data in Python.\n", "    imgs, labels = get_label_img('%s/test-*' % data_path, n=rows*cols)\n", "    # Create input_fn from Python data.\n", "    input_fn = tf.estimator.inputs.numpy_input_fn({'img_64': imgs}, shuffle=False)\n", "    # Get predictions.\n", "    predict_iter = estimator.predict(input_fn=input_fn, predict_keys=predict_keys)\n", "    _, axs = pyplot.subplots(rows, cols, figsize=(cols*3, rows*3))\n", "    for (i, prediction) in enumerate(predict_iter):\n", "        # Plot image, predicted label + correct label.\n", "        predicted = np.argmax(prediction['probabilities'])\n", "        label = labels[i, 0]\n", "        title = '%s? %s!' % (classes[predicted], classes[label])\n", "        ax = axs[i//cols][i%cols]\n", "        show_img(imgs[i], title, ax)\n", "        ax.title.set_position([0.5, 1.0])\n", "\n", "show_predictions(linear_estimator)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Let's use another canned estimator : The \"DNNClassifier\" (where DNN stands\n", "# for \"deep neural network\"). Since we're using the same estimator interface,\n", "# we can use the same input_fn() as before.\n", "\n", "# YOUR ACTION REQUIRED:\n", "# Try to find a configuration that outperforms our linear classifier.\n", "steps = \n", "hidden_units = \n", "\n", "dnn_estimator  = tf.estimator.DNNClassifier(\n", "    hidden_units=hidden_units,\n", "    feature_columns=feature_columns,\n", "    n_classes=len(classes))\n", "dnn_estimator.train(input_fn=make_input_fn('%s/train-*' % data_path),\n", "                    steps=steps)\n", "dnn_estimator.evaluate(input_fn=make_input_fn('%s/eval-*' % data_path),\n", "                       steps=100)"], "outputs": [], "metadata": {}}, {"source": ["# 3 Custom convolutional classifier \u2013\u00a0bonus!<a name=\"_3 custom convolutional classifier \u2013\u00a0bonus!\"></a><a name=\"_3 custom convolutional classifier \u2013\u00a0bonus!\"></a>"], "cell_type": "markdown", "metadata": {}}, {"source": ["In the previous section we used a \"canned\" linear estimator that\n", "computed the predictions from the pixel values directly by logistic\n", "regression.\n", "\n", "Of course we know that the pixel values are not randomly distributed,\n", "but actually form a two-dimensional image and we can leverage our\n", "understanding of the data by using\n", "[2D convolutions](https://en.wikipedia.org/wiki/Multidimensional_discrete_convolution).\n", "\n", "We will still be using the `tf.estimator` interface, but this time we\n", "specify the computational graph that computes the predictions from the\n", "raw pixel values by hand, using 2D convolutions and max pooling (max pooling\n", "is used to reduce the convoluted image's spatial dimensions)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_get_logits_img.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Define a function that computes \"logits\" from features.\n", "# The \"logits\" are unbound numbers that will be used as the\n", "# input to the softmax function (which is basically a sigmoid\n", "# function extended to more than two classes). The more positive\n", "# a logit, the closer to one the corresponding probability that\n", "# is the output of the softmax.\n", "# https://en.wikipedia.org/wiki/Softmax_function\n", "\n", "def get_logits_img(features, n_classes, mode, params):\n", "    \"\"\"Computes logits for provided features.\n", "\n", "    Args:\n", "      features: A dictionary of tensors that are the features\n", "          and whose first dimension is batch (as returned by input_fn).\n", "      n_classes: Number of classes from which to predict (i.e. the number\n", "          of different values in the \"labels\" tensor returned by the\n", "          input_fn).\n", "      mode: A tf.estimator.ModeKeys.\n", "      params: Hyper parameters: \"convs\" specifying the configuration of the\n", "          convolutions, and \"hidden\" specifying the configuration of the\n", "          dense layers after the convolutions.\n", "\n", "    Returns:\n", "      The logits tensor with shape=[batch, n_classes].\n", "    \"\"\"\n", "    # The parameter \"convs\" specifies (kernel, stride, filters)\n", "    # of successive convolution layers.\n", "    convs = params.get('convs', ((10, 4, 32), (5, 4, 64)))\n", "    # The parameter \"hidden\" specifies the number of neurons of\n", "    # successive fully connected layers (after convolution).\n", "    hidden = params.get('hidden', (256,))\n", "    # The function tf.layers.conv2d expects the tensor to have format\n", "    # [batch, height, width, channels] -- since our \"img_64\" tensor\n", "    # has format [batch, height, width], we need to expand the tensor\n", "    # to get [batch, height, width, channels=1].\n", "    last_layer = tf.expand_dims(features['img_64'], axis=3)\n", "    # We start with dims=width=height=64 and filters=channels=1 and then\n", "    # successively reduce the number of dimensions while increasing the\n", "    # number of filters in every convolutional/maxpooling layer.\n", "    dim = 64\n", "    filters = 1\n", "    for kernel, stride, filters in convs:\n", "        conv = tf.layers.conv2d(\n", "            inputs=last_layer, filters=filters, kernel_size=[kernel, kernel],\n", "            padding='same', activation=tf.nn.relu)\n", "        last_layer = tf.layers.max_pooling2d(\n", "            inputs=conv, pool_size=[stride, stride], strides=stride)\n", "        dim /= stride\n", "    # \"Flatten\" the last layer to get shape [batch, *]\n", "    last_layer = tf.reshape(last_layer, [-1, filters * dim * dim])\n", "    # Add some fully connected layers.\n", "    for units in hidden:\n", "        dense = tf.layers.dense(inputs=last_layer, units=units,\n", "                                activation=tf.nn.relu)\n", "        # Regularize using dropout.\n", "        training = mode == tf.estimator.ModeKeys.TRAIN\n", "        last_layer = tf.layers.dropout(inputs=dense, rate=0.4,\n", "                                       training=training)\n", "    # Finally return logits that is activation of neurons in last layer.\n", "    return tf.layers.dense(inputs=last_layer, units=n_classes)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_make_model_fn.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Warning : Boilerplate code cell...\n", "\n", "# This cell defines a function that connects the \"get_logits_fn\"\n", "# to the estimator interface and adds some useful output for our problem.\n", "# By specifying different parameters, the same function can be reused\n", "# in section 5 where we use a recurrent neural network to compute the\n", "# logits.\n", "\n", "def make_model_fn(get_logits_fn, n_classes):\n", "    \"\"\"Creates a model_fn.\n", "\n", "    Args:\n", "      get_logits_fn: Function that computes logits from features.\n", "      n_classes: Number of classes.\n", "\n", "    Returns:\n", "      A model_fn to be used with an estimator.\n", "    \"\"\"\n", "\n", "    def model_fn(features, labels, mode, params):\n", "        \"\"\"The model_fn is passed as an argument to the estimator.\n", "\n", "        Args:\n", "          features: Dictionary mapping feature names to feature tensors.\n", "          labels: Optional labels (`None` during inference).\n", "          mode: A `tf.estimator.ModeKeys`.\n", "          params: Optional dictionary of hyper parameters.\n", "\n", "        Returns:\n", "          A `tf.estimator.EstimatorSpec`.\n", "        \"\"\"\n", "\n", "        # Create logits from features using RNN.\n", "        logits = get_logits_fn(features, n_classes=n_classes, mode=mode,\n", "                               params=params)\n", "\n", "        # Convert logits to probabilities.\n", "        probabilities = tf.nn.softmax(logits)\n", "        # Extract class with highest probability.\n", "        predictions = tf.argmax(probabilities, axis=1)\n", "\n", "        onehot_labels = loss = train_op = eval_metric_ops = None\n", "        if labels is not None:\n", "            onehot_labels = tf.one_hot(tf.squeeze(labels), n_classes)\n", "            loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\n", "\n", "        if mode == tf.estimator.ModeKeys.TRAIN:\n", "            # Compute loss.\n", "            global_step = tf.train.get_global_step()\n", "            # Minimize.\n", "            train_op = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n", "            tf.summary.scalar('loss', loss)\n", "            # Output number of parameters for educational purposes.\n", "            trainable_params = 0\n", "            for var in tf.trainable_variables():\n", "                tf.logging.info('Variable \"%s\" : %s.', var.name,\n", "                                var.get_shape().as_list())\n", "                trainable_params += np.prod(var.get_shape().as_list())\n", "            tf.logging.info('Total params : %d.', trainable_params)\n", "\n", "        if mode == tf.estimator.ModeKeys.EVAL:\n", "            # Report accuracy when evaluating.\n", "            eval_metric_ops = {\n", "                'accuracy': tf.metrics.accuracy(labels, predictions),\n", "            }\n", "\n", "        return tf.estimator.EstimatorSpec(\n", "            loss=loss,\n", "            mode=mode,\n", "            predictions={\n", "                'probabilities': probabilities,\n", "                'predictions': predictions,\n", "            },\n", "            export_outputs={\n", "                'prediction': tf.estimator.export.PredictOutput(outputs={\n", "                    'probabilities': probabilities,\n", "                    'predictions': predictions,\n", "                }),\n", "            },\n", "            train_op=train_op,\n", "            eval_metric_ops=eval_metric_ops,\n", "        )\n", "\n", "    return model_fn"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%run -i _derived/4_get_logits_img.py\n", "%run -i _derived/4_make_model_fn.py\n", "\n", "# Instead of creating a tf.estimator.LinearClassifier, we now create\n", "# a tf.estimator.Estimator and specify our custom model_fn that will\n", "# be used by the estimator to compute the predictions from the features.\n", "run_config = tf.estimator.RunConfig(save_summary_steps=10)\n", "model_fn = make_model_fn(get_logits_fn=get_logits_img, n_classes=len(classes))\n", "cnn_estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n", "# -- from here on, the code is exactly the same as in the previous section ..."], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Note: You can get detailed information about the training by pointing a\n", "# Tensorboard instance to the training directory.\n", "#\n", "# You start Tensorboard via a terminal, for example by clicking on \"New\"\n", "# -> \"Terminal\" in the Jupyter directory view (this also works when\n", "# Jupyter server is running inside a Docker image).\n", "#\n", "# The following command will start Tensorboard and show all experimental\n", "# data below the \"/tmp\" directory (which is where new model directories\n", "# are created if we do not specify a model path):\n", "#\n", "# $ tensorboard --logdir /tmp/\n", "#\n", "# And then view the Tensorboard in your browser:\n", "# http://localhost:6006\n", "\n", "# Bonus challenge (don't do this during the workshop) : Try to increase the\n", "# number of training steps, batch_size, and/or parameters (hint: check out\n", "# the \"params\" argument when creating the estimator in the previous cell)\n", "# -- what accuracy can achieve with this convolutional network?\n", "input_fn = make_input_fn('%s/train-*' % data_path, batch_size=100)\n", "cnn_estimator.train(input_fn=input_fn, steps=100)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["cnn_estimator.evaluate(input_fn=make_input_fn('%s/eval-*' % data_path),\n", "                       steps=1)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["input_fn = make_input_fn('%s/test-*' % data_path, batch_size=1)\n", "show_predictions(cnn_estimator, predict_keys='probabilities')"], "outputs": [], "metadata": {}}, {"source": ["# 4 Keras \u2013\u00a0bonus!<a name=\"_4 keras \u2013\u00a0bonus!\"></a><a name=\"_4 keras \u2013\u00a0bonus!\"></a>"], "cell_type": "markdown", "metadata": {}}, {"source": ["\"Keras is a high-level neural networks API, written in Python and capable of\n", "running on top of TensorFlow, CNTK, or Theano. It was developed with a focus\n", "on enabling fast experimentation. Being able to go from idea to result with\n", "the least possible delay is key to doing good research.\"\n", "(from https://keras.io/)\n", "\n", "In this section we reconstruct the same convolutional model as in the last\n", "section using Keras and then convert the Keras model to an estimator so we\n", "can use the same input functions reading data in a streaming fashion from our\n", "data on disk.\n", "\n", "More useful links about Keras (not needed for this section):\n", "\n", "- https://keras.io/models/about-keras-models/\n", "- https://keras.io/models/sequential/\n", "- https://keras.io/models/model/\n", "- [Converting Keras model to Tensorflow estimator](https://cloud.google.com/blog/big-data/2017/12/new-in-tensorflow-14-converting-a-keras-model-to-a-tensorflow-estimator)\n", "- [Example using functional API with input_fn](https://github.com/keras-team/keras/blob/master/examples/mnist_tfrecord.py)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Keras is part of Tensorflow\n", "from tensorflow import keras\n", "\n", "# The model is specified as a succession of layers.\n", "# The layers here specify exactly the same model as in the previous section.\n", "model = keras.models.Sequential([\n", "    # Note that we need to name the first layer (to match features, see next\n", "    # cell) and specify the input shape.\n", "    keras.layers.Conv2D(filters=32, kernel_size=10, activation='relu',\n", "                        padding='same', name='firstlayer',\n", "                        input_shape=(64, 64, 1)),\n", "    keras.layers.MaxPooling2D(pool_size=4),\n", "    keras.layers.Conv2D(filters=64, kernel_size=5, activation='relu',\n", "                        padding='same'),\n", "    keras.layers.MaxPooling2D(pool_size=4),\n", "    keras.layers.Flatten(),\n", "    keras.layers.Dense(units=256, activation='relu'),\n", "    # We also need to name the output layer.\n", "    tf.keras.layers.Dense(units=len(classes), activation='softmax',\n", "                          name='labels'),\n", "])\n", "model.compile(loss='categorical_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])\n", "# Neat :-)\n", "model.summary()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Helper function to wrap input_fn().\n", "def keras_make_input_fn(*args, **kwargs):\n", "    def wrapper():\n", "        input_fn = make_input_fn(*args, **kwargs)\n", "        features, labels = input_fn()\n", "        # We need to specify which feature is used as the input to\n", "        # which layer. In our case we have a single input feature\n", "        # that is read by layer \"firstlayer\". Note that we also need\n", "        # to add an additional dimension to get shape=(64, 64, 1).\n", "        features = {\n", "            'firstlayer_input': tf.expand_dims(features['img_64'], axis=3),\n", "        }\n", "        # Labels are expected in one_hot format.\n", "        labels = tf.one_hot(tf.squeeze(labels), 10)\n", "        return features, labels\n", "    return wrapper\n", "\n", "# From now on we proceed exactly as before...\n", "keras_estimator = keras.estimator.model_to_estimator(model)\n", "input_fn = keras_make_input_fn('%s/train-*' % data_path, batch_size=100)\n", "keras_estimator.train(input_fn=input_fn, steps=100)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["keras_estimator.evaluate(input_fn=keras_make_input_fn('%s/eval-*' % data_path),\n", "                         steps=1)"], "outputs": [], "metadata": {}}, {"source": ["# 5 Recurrent neural network \u2013\u00a0bonus!\n", "\n", "So far we have been processing two dimensional image data. That\n", "data can be converted nicely into a dense tensor that is then the\n", "input layer of a linear classifier or convolutional network.\n", "\n", "If we want to process the raw stroke data, we need a different\n", "network architecture that reads in coordinate by coordinate, while\n", "updating its internal state and finally outputs a prediction --\n", "a **recurrent network**.\n", "\n", "Explaining the architecture for recurrent networks (specifically,\n", "we use a LSTM in this section) is out of the scope of this workshop,\n", "please read the following articles if you want to know more about the\n", "architecture:\n", "\n", "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n", "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n", "\n", "Using recurrent networks requires some tensor shape black magic and\n", "this is the main focus of this section."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Load stroke data generated in 1_qd_data (bonus section):\n", "stroke_data_path = '../data/dataset_stroke'\n", "!ls -l -h $stroke_data_path"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Label classes...\n", "classes = open('%s/labels.txt' % stroke_data_path).read().splitlines()\n", "print '%d label classes:\\n' % len(classes)\n", "for i, label in enumerate(classes):\n", "    print '%d -> %s' % (i, label)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Some remarks on \"sparse tensors\": Conceptually, these\n", "# tensors are an efficient representation of a tensor that\n", "# has mostly \"0\" values. In the example below we initialize\n", "# a sparse tensor by specifying only non-\"0\" values and then\n", "# print the tensor as well as its dense representation.\n", "\n", "# If we read \"variable length\" tensors from disk then these\n", "# tensors will also be represented as sparse tensors (although\n", "# the tensors might be \"dense\" in a sense that they have very\n", "# few \"0\" values).\n", "\n", "with tf.Graph().as_default():\n", "    # Only specify non-\"0\" values of 3x3 diagnal matrix.\n", "    sparse = tf.SparseTensor(indices=[[0, 0], [1, 1], [2, 2]], values=[1, 2, 3],\n", "                             dense_shape=[3, 3])\n", "    # Convert to dense representation. Note that we can specify any\n", "    # \"dense_shape\" (resulting dense tensor is zero padded).\n", "    dense = tf.sparse_to_dense(sparse.indices, sparse.dense_shape,\n", "                               sparse.values)\n", "    with tf.Session() as sess:\n", "        print sparse.eval()\n", "        print dense.eval()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_convert_sparse.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Now let's define a helper function that limits variable length\n", "# sparse tensors to a maximum length and converts them to dense\n", "# tensors. We need to convert sparse tensors to dense tensors before\n", "# we can use them as input in the recurrent neural network.\n", "\n", "def convert_sparse(sparse, max_len):\n", "    \"\"\"Converts batched sparse tensor to dense tensor with specified size.\n", "    \n", "    Args:\n", "      sparse: tf.SparseTensor instance of shape=[n].\n", "      max_len: Truncates / zero-pads the dense tensor the specified max_len.\n", "    \"\"\"\n", "    # Convert to dense tensor.\n", "    dense = tf.sparse_to_dense(sparse.indices, sparse.dense_shape,\n", "                               sparse.values)\n", "    # Discard values above max_len.\n", "    dense = dense[:max_len]\n", "    # Zero-pad if length < max_len.\n", "    dense = tf.pad(dense, [[0, max_len - tf.shape(dense)[0]]])\n", "    return dense"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%run -i _derived/4_convert_sparse.py\n", "\n", "with tf.Graph().as_default():\n", "    # Manually define sparse X-coordinates [1,2,3,4,5].\n", "    # Note that our stroke coordinate \"sparse tensors\" have a single dimension\n", "    # and do not contain any zeros at all...\n", "    stroke_x = tf.SparseTensor(\n", "        indices=[[0], [1], [2], [3], [4]],\n", "        values=[1, 2, 3, 4, 5],\n", "        dense_shape=[5])\n", "    # Extract both shorter and longer dense tensor.\n", "    dense_short = convert_sparse(stroke_x, max_len=3)\n", "    dense_long = convert_sparse(stroke_x, max_len=10)\n", "    with tf.Session() as sess:\n", "        print dense_short.eval()\n", "        print dense_long.eval()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_input_fn_stroke.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Because the data is stored in a different format (strokes instead of pixels)\n", "# we need a new input_fn.\n", "\n", "# Maximum number of points in concatenated strokes.\n", "MAX_LEN = 256\n", "\n", "# Because every drawing has a different number of points, we use \"VarLenFeature\"\n", "# and not \"FixedLenFeature\" for the stroke data. This will create\n", "# \"SparseTensor\".\n", "feature_spec = {\n", "    'stroke_x': tf.VarLenFeature(dtype=tf.float32),\n", "    'stroke_y': tf.VarLenFeature(dtype=tf.float32),\n", "    'stroke_z': tf.VarLenFeature(dtype=tf.float32),\n", "    'stroke_len': tf.FixedLenFeature([], tf.int64),\n", "    'label': tf.FixedLenFeature([], tf.int64),\n", "}\n", "\n", "def parse_example_stroke(serialized_example):\n", "    features = tf.parse_single_example(serialized_example, feature_spec)\n", "    label = features.pop('label')\n", "\n", "    # The we create a 'stroke' tensor with shape [3, MAX_LEN] where the first\n", "    # dimension indicates whether the values are X, Y, or Z coordinates.\n", "    stroke = tf.stack([\n", "        convert_sparse(features['stroke_x'], max_len=MAX_LEN),\n", "        convert_sparse(features['stroke_y'], max_len=MAX_LEN),\n", "        convert_sparse(features['stroke_z'], max_len=MAX_LEN),\n", "    ])\n", "\n", "    # Also truncate the \"stroke_len\" to MAX_LEN if needed.\n", "    stroke_len = tf.minimum(tf.cast(MAX_LEN, tf.int64), features['stroke_len'])\n", "\n", "    return dict(stroke=stroke, stroke_len=stroke_len), label\n", "\n", "# Copied from above Section \"1.2 Reading the data using Tensorflow\"\n", "def make_input_fn_stroke(files_pattern, batch_size=100):\n", "    def input_fn():\n", "        dataset = tf.data.TFRecordDataset(tf.gfile.Glob(files_pattern))\n", "        dataset = dataset.map(parse_example_stroke).batch(batch_size)\n", "        dataset = dataset.shuffle(buffer_size=5*batch_size).repeat()\n", "        features, labels = dataset.make_one_shot_iterator().get_next()\n", "        return features, labels\n", "    return input_fn"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Read some examples with above input_fn and plot data.\n", "\n", "%run -i _derived/4_input_fn_stroke.py\n", "\n", "# (Modified code from section 1.1 and 1.2)\n", "\n", "def show_stroke_img(stroke, label, ax=None):\n", "    \"\"\"Plots stroke data.\n", "\n", "    Args:\n", "      stroke: Array of shape=[3, n] where the second dimension\n", "          is time and the first dimension indicates X/Y coordinates\n", "          and Z-dimension that is set to 1 when a stroke ends and\n", "          0 otherwise (the array actually represents an array of\n", "          concatenated strokes and the Z-dimension is needed to tell\n", "          the individual strokes apart).\n", "    \"\"\"\n", "    ax = ax if ax else pyplot.gca()\n", "    xy = stroke[:2, :].cumsum(axis=1)\n", "    ax.plot(*xy)\n", "    # Plot all the strokes, including connecting line between strokes.\n", "    pxy = xy[:, stroke[2] != 0]\n", "    # Red dots mark end of individual strokes.\n", "    ax.plot(pxy[0], pxy[1], 'ro')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(label)\n", "\n", "batch_size = 5\n", "input_fn = make_input_fn_stroke(files_pattern='%s/train-*' % stroke_data_path,\n", "                                batch_size=batch_size)\n", "with tf.Graph().as_default():\n", "    features, labels = input_fn()\n", "    with tf.train.MonitoredSession() as sess:\n", "        strokes_, labels_ = sess.run([features['stroke'], labels])\n", "        pyplot.figure(figsize=(10, 2))\n", "        for i in range(batch_size):\n", "            ax = pyplot.subplot(1, batch_size, i+1)\n", "            show_stroke_img(strokes_[i], classes[labels_[i]], ax)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_get_nth.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# Another helper function: This function will return the \"nth element\"\n", "# with different \"n\" for every element in the batch.\n", "# We will later need this function to get the prediction in the output\n", "# of the dynamic_rnn.\n", "\n", "def get_nth(tensor, ns, last_dim):\n", "    \"\"\"Tensor has shape [batch_size, max_len, last_dim].\"\"\"\n", "    shape = tf.shape(tensor)\n", "    batch_size, max_len = shape[0], shape[1]\n", "    # Flatten first two dimensions.\n", "    tensor = tf.reshape(tensor, [-1, last_dim])\n", "    # Calculate indices within flattened tensor.\n", "    idxs = tf.range(0, batch_size) * max_len + (tf.cast(ns, tf.int32) - 1)\n", "    # Return nth elements.\n", "    # TODO get rid of error UserWarning : https://stackoverflow.com/questions/35892412\n", "    return tf.gather(tensor, idxs)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%run -i _derived/4_get_nth.py\n", "\n", "with tf.Graph().as_default(), tf.Session(graph=tf.Graph()) as sess:\n", "    # Define a batch with shape=[batch_size=2, max_len=5, last_dim=2]\n", "    batch = tf.constant([\n", "        # First tensor in batch (shape=[5, 2])\n", "        [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]],\n", "        # Second tensor in batch (shape=[5, 2])\n", "        [[-1, -1], [-2, -2], [-3, -3], [-4, -4], [-5, -5]],\n", "    ])\n", "    # Return the second and third element.\n", "    lens = tf.constant([2, 3], dtype=tf.int64)\n", "    print get_nth(batch, lens, last_dim=2).eval()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%%writefile _derived/4_get_logits_stroke.py\n", "# (Written into separate file for sharing with cloud code.)\n", "\n", "# This function creates creates the logits fro the stroke features.\n", "def get_logits_stroke(features, n_classes, mode, params):\n", "    \"\"\"Computes logits for provided features.\n", "\n", "    Args:\n", "      features: A dictionary of tensors that are the features\n", "          and whose first dimension is batch (as returned by input_fn).\n", "      n_classes: Number of classes from which to predict (i.e. the number\n", "          of different values in the \"labels\" tensor returned by the\n", "          input_fn).\n", "      mode: A tf.estimator.ModeKeys.\n", "      params: Hyper parameters: \"cell_size\" specifying the state size of\n", "          the LSTM cells, and \"hidden\" specifying the configuration of the\n", "          dense layers after recurrent network.\n", "\n", "    Returns:\n", "      The logits tensor with shape=[batch, n_classes].\n", "    \"\"\"\n", "\n", "    cell_size = params.get('cell_size', 256)\n", "    hidden = params.get('hidden', ())\n", "    \n", "    # First we convert our data from \"coords major\" to \"time major\",\n", "    # as required by the dynamic_rnn API.\n", "    # [batch, coords, time] -> [batch, time, coords]\n", "    stroke = tf.transpose(features['stroke'], perm=[0, 2, 1])\n", "    stroke_len = features['stroke_len']\n", "\n", "    # Construct a bi-directional dynamic recurrent NN with LSTM\n", "    # cells.\n", "    outputs, states = tf.nn.bidirectional_dynamic_rnn(\n", "            cell_fw=tf.nn.rnn_cell.LSTMCell(cell_size),\n", "            cell_bw=tf.nn.rnn_cell.LSTMCell(cell_size),\n", "            inputs=stroke,\n", "            sequence_length=stroke_len,\n", "            dtype=tf.float32,\n", "    )\n", "    # Use helper function from last cell to extract RNN output values.\n", "    outputs = tf.concat((get_nth(outputs[0], stroke_len, last_dim=cell_size),\n", "                         get_nth(outputs[1], stroke_len, last_dim=cell_size)),\n", "                        axis=1)\n", "\n", "    # Add fully connected layers on top.\n", "    for units in hidden:\n", "        outputs = tf.layers.dense(inputs=outputs, units=units,\n", "                                  activation=tf.nn.relu)\n", "\n", "    # Logits are activations of last fully connected layer.\n", "    return tf.layers.dense(inputs=outputs, units=n_classes)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%run -i _derived/4_get_logits_stroke.py\n", "\n", "# Same as before : Create estimator, train, evaluate.\n", "# Note that we need *much* more time and/or CPU power to get\n", "# decent results with the RNN, but eventually it will outperform\n", "# the convolutional classifier...\n", "\n", "# Use make_model_fn from section \"3 Custom convolutional classifier\"\n", "model_fn = make_model_fn(get_logits_fn=get_logits_stroke, n_classes=len(classes))\n", "config = tf.estimator.RunConfig(save_summary_steps=1)\n", "rnn_estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\n", "\n", "# We use smaller batch size to keep memory usage below ~1G (the\n", "# RNN has a large memory footprint because of the temporal unrolling).\n", "# To get good performance with this type of network we need much longer\n", "# training, which is impractical in a notebook. See next notebook\n", "# \"5_qd_cloud\" that describes how to train the network on Google Cloud\n", "# Environment using Cloud ML...\n", "input_fn = make_input_fn_stroke('%s/train-*' % stroke_data_path, batch_size=10)\n", "rnn_estimator.train(input_fn=input_fn, steps=100)\n", "input_fn = make_input_fn_stroke('%s/eval-*' % stroke_data_path, batch_size=10)\n", "rnn_estimator.evaluate(input_fn=input_fn, steps=10)"], "outputs": [], "metadata": {}}, {"source": ["# A References\n", "\n", "- https://www.tensorflow.org/versions/master/get_started/feature_columns\n", "- https://arxiv.org/abs/1704.03477 : A Neural Representation of Sketch Drawings\n", "- https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd : Nice tutorial explaining convolutions, recurrent networks, and some deep learning tricks \u2013\u00a0Challenge: try to improve the models in this notebook with the techniques described in this presentation!"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.12", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}