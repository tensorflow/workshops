{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie-descriptions-tfhub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "feFbqZpP1soY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a text classification model with TF Hub\n",
        "\n",
        "In this notebook, we'll walk you through building a model to predict the genres of a movie given its description. The emphasis here is not on accuracy, but instead how to use TF Hub layers in a text classification model.\n",
        "\n",
        "To start, import the necessary dependencies for this project."
      ]
    },
    {
      "metadata": {
        "id": "rOEllRxGQ_me",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import pickle\n",
        "import urllib\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvwN2Jkx2CdU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The dataset\n",
        "\n",
        "We need a lot of text inputs to train our model. For this model we'll use [this awesome movies dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) from Kaggle. To simplify things I've made the `movies_metadata.csv` file available in a public Cloud Storage bucket so we can download it with `wget`. I've preprocessed the dataset already to limit the number of genres we'll use for our model, but first let's take a look at the original data so we can see what we're working with."
      ]
    },
    {
      "metadata": {
        "id": "YObfZBenyfMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the data from GCS\n",
        "!wget 'https://storage.googleapis.com/movies_data/movies_metadata.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFKB0Bw62xW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll convert the dataset to a Pandas dataframe and print the first 5 rows. For this model we're only using 2 of these columns: `genres` and `overview`."
      ]
    },
    {
      "metadata": {
        "id": "NaZiKWtGyoQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('movies_metadata.csv')\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBLcNSE_7Icv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data for our model\n",
        "\n",
        "I've done some preprocessing to limit the dataset to the top 9 genres, and I've saved the Pandas dataframes as public [Pickle](https://docs.python.org/3/library/pickle.html) files in GCS. Here we download those files. The resulting `descriptions` and `genres` variables are Pandas Series containing all descriptions and genres from our dataset respectively."
      ]
    },
    {
      "metadata": {
        "id": "rzjJuKhir-PH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve('https://storage.googleapis.com/bq-imports/descriptions.p', 'descriptions.p')\n",
        "urllib.request.urlretrieve('https://storage.googleapis.com/bq-imports/genres.p', 'genres.p')\n",
        "\n",
        "descriptions = pickle.load(open('descriptions.p', 'rb'))\n",
        "genres = pickle.load(open('genres.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUypuN818T_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting our data\n",
        "When we train our model, we'll use 80% of the data for training and set aside 20% of the data to evaluate how our model performed."
      ]
    },
    {
      "metadata": {
        "id": "_nticMcj1alW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_size = int(len(descriptions) * .8)\n",
        "\n",
        "train_descriptions = descriptions[:train_size].astype('str')\n",
        "train_genres = genres[:train_size]\n",
        "\n",
        "test_descriptions = descriptions[train_size:].astype('str')\n",
        "test_genres = genres[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmZ9iqK88nSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Formatting our labels\n",
        "When we train our model we'll provide the labels (in this case genres) associated with each movie. We can't pass the genres in as strings directly, we'll transform them into multi-hot vectors. Since we have 9 genres, we'll have a 9 element vector for each movie with 0s and 1s indicating which genres are present in each description."
      ]
    },
    {
      "metadata": {
        "id": "bouv0R-D7J45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = MultiLabelBinarizer()\n",
        "encoder.fit_transform(train_genres)\n",
        "train_encoded = encoder.transform(train_genres)\n",
        "test_encoded = encoder.transform(test_genres)\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "# Print all possible genres and the labels for the first movie in our training dataset\n",
        "print(encoder.classes_)\n",
        "print(train_encoded[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ir8ez0K_9sYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create our TF Hub embedding layer\n",
        "[TF Hub]() provides a library of existing pre-trained model checkpoints for various kinds of models (images, text, and more) In this model we'll use the TF Hub `universal-sentence-encoder` module for our pre-trained word embeddings. We only need one line of code to instantiate module. When we train our model, it'll convert our array of movie description strings to embeddings. When we train our model, we'll use this as a feature column.\n"
      ]
    },
    {
      "metadata": {
        "id": "PWuNUXq7a-7p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "description_embeddings = hub.text_embedding_column(\"descriptions\", module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\", trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vscf4Fo-iI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Instantiating our DNNEstimator Model\n",
        "The first parameter we pass to our DNNEstimator is called a head, and defines the type of labels our model should expect. Since we want our model to output multiple labels, weâ€™ll use multi_label_head here. Then we'll convert our features and labels to numpy arrays and instantiate our Estimator. `batch_size` and `num_epochs` are hyperparameters - you should experiment with different values to see what works best on your dataset."
      ]
    },
    {
      "metadata": {
        "id": "c0Vsmu9O21je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_label_head = tf.contrib.estimator.multi_label_head(\n",
        "    num_classes,\n",
        "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mTpWD_Q8GKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = {\n",
        "  \"descriptions\": np.array(train_descriptions).astype(np.str)\n",
        "}\n",
        "labels = np.array(train_encoded).astype(np.int32)\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(features, labels, shuffle=True, batch_size=32, num_epochs=25)\n",
        "estimator = tf.contrib.estimator.DNNEstimator(\n",
        "    head=multi_label_head,\n",
        "    hidden_units=[64,10],\n",
        "    feature_columns=[description_embeddings])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ak1cZPZ_ZYM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and serving our model \n",
        "To train our model, we simply call `train()` passing it the input function we defined above. Once our model is trained, we'll define an evaluation input function similar to the one above and call `evaluate()`. When this completes we'll get a few metrics we can use to evaluate our model's accuracy.\n"
      ]
    },
    {
      "metadata": {
        "id": "jmtvJ5o3Olcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimator.train(input_fn=train_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMgti0YmJO7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define our eval input_fn and run eval\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(test_descriptions).astype(np.str)}, test_encoded.astype(np.int32), shuffle=False)\n",
        "estimator.evaluate(input_fn=eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcPyCfmWABVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating predictions on new data\n",
        "Now for the most fun part! Let's generate predictions on movie descriptions our model hasn't seen before. We'll define an array of 3 new description strings (the comments indicate the correct genres) and create a `predict_input_fn`. Then we'll display the top 2 genres along with their confidence percentages for each of the 3 movies."
      ]
    },
    {
      "metadata": {
        "id": "ixlCKF6NEkTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test our model on some raw description data\n",
        "raw_test = [\n",
        "    \"An examination of our dietary choices and the food we put in our bodies. Based on Jonathan Safran Foer's memoir.\", # Documentary\n",
        "    \"After escaping an attack by what he claims was a 70-foot shark, Jonas Taylor must confront his fears to save those trapped in a sunken submersible.\", # Action, Adventure\n",
        "    \"A teenager tries to survive the last week of her disastrous eighth-grade year before leaving to start high school.\", # Comedy\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHpMIWFsE4OB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predict_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(raw_test).astype(np.str)}, shuffle=False)\n",
        "results = estimator.predict(predict_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMVzrHpPDvoy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display predictions\n",
        "for movie_genres in results:\n",
        "  top_2 = movie_genres['probabilities'].argsort()[-2:][::-1]\n",
        "  for genre in top_2:\n",
        "    text_genre = encoder.classes_[genre]\n",
        "    print(text_genre + ': ' + str(round(movie_genres['probabilities'][genre] * 100, 2)) + '%')\n",
        "  print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CfZTfK-e7MJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}