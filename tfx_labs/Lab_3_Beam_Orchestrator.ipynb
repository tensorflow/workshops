{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX Lab 3 â€“ On-Prem with Beam Orchestrator",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "QNxEZK2pPA1P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNxEZK2pPA1P",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright &copy; 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JwKPOmN2-15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW",
        "colab_type": "text"
      },
      "source": [
        "# Using the Apache Beam Orchestrator for TFX\n",
        "\n",
        "This notebook demonstrates how to use Apache Beam as an orchestrator for TFX. Pipelines are also created in Apache Beam, which means that Beam sequences tasks according to the dependencies of each task, running each task as its dependencies are met.  Beam is also highly scalable and runs tasks in parallel in a distributed environment.  That makes Beam very powerful as an orchestrator for other pipelines, including TFX.\n",
        "\n",
        "When using the InteractiveContext in a notebook, running each cell orchestrates the creation and running of each of the components in the TFX pipeline.  When using a separate orchestrator, as in this example, the components are only run once the TFX pipeline DAG has been defined and the orchestrator has been triggered to start an execution run.\n",
        "\n",
        "In this example you will define all the supporting code for the TFX components before instantiating the components and running the TFX pipeline using an Apache Beam orchestrator.  This is the pattern which is typically used in a production deployment of TFX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "First, we install the necessary packages, download data, import modules and set up paths.\n",
        "\n",
        "### Install TFX and TensorFlow\n",
        "\n",
        "> #### Note\n",
        "> Because of some of the updates to packages you must use the button at the bottom of the output of this cell to restart the runtime.  Following restart, you should rerun this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zhcJBLoMXQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U \\\n",
        "  tensorflow==2.0.0 \\\n",
        "  tfx==0.15.0rc0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q",
        "colab_type": "text"
      },
      "source": [
        "### Import packages\n",
        "We import necessary packages, including standard TFX component classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tfx\n",
        "from tfx.components.evaluator.component import Evaluator\n",
        "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.model_validator.component import ModelValidator\n",
        "from tfx.components.pusher.component import Pusher\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.component import Trainer\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.proto import evaluator_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.utils.dsl_utils import external_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfS6otsARvJC",
        "colab_type": "text"
      },
      "source": [
        "Check the versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZY7Pnoxmoe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX",
        "colab_type": "text"
      },
      "source": [
        "### Download example data\n",
        "We download the sample dataset for use in our TFX pipeline.  We're working with a variant of the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/online+news+popularity) dataset, which summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict how popular the article will be on social networks. Specifically, in the original dataset the objective was to predict the number of times each article will be shared on social networks. In this variant, the goal is to predict the article's popularity percentile. For example, if the model predicts a score of 0.7, then it means it expects the article to be shared more than 70% of all articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywX6OUEhAqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the example data.\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/ageron/open-datasets/master/' \\\n",
        "   'online_news_popularity_for_course/online_news_popularity_for_course.csv'\n",
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXu5IR6dSDwJ",
        "colab_type": "text"
      },
      "source": [
        "Take a quick look at the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqn4wST2Bex5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head {_data_filepath}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufJKQ6OvkJlY",
        "colab_type": "text"
      },
      "source": [
        "### Set up pipeline paths\n",
        "\n",
        "We create the filenames for the Python modules for the Transform and Trainer components, and a directory for the Serving model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad5JLpKbf6sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up paths.\n",
        "_constants_module_file = 'online_news_constants.py'\n",
        "_transform_module_file = 'online_news_transform.py'\n",
        "_trainer_module_file = 'online_news_trainer.py'\n",
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(),\n",
        "                                  'serving_model/online_news_simple')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TlleGZPSqdn",
        "colab_type": "text"
      },
      "source": [
        "Define some constants and functions for both the `Transform` component and the `Trainer` component.  Define them in a Python module, in this case saved to disk using the `%%writefile` magic command since you are working in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpU9-JNXw-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_constants_module_file}\n",
        "\n",
        "DENSE_FLOAT_FEATURE_KEYS = [\n",
        "    \"timedelta\", \"n_tokens_title\", \"n_tokens_content\",\n",
        "    \"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
        "    \"n_hrefs\", \"n_self_hrefs\", \"n_imgs\", \"n_videos\", \"average_token_length\",\n",
        "    \"n_keywords\", \"kw_min_min\", \"kw_max_min\", \"kw_avg_min\", \"kw_min_max\",\n",
        "    \"kw_max_max\", \"kw_avg_max\", \"kw_min_avg\", \"kw_max_avg\", \"kw_avg_avg\",\n",
        "    \"self_reference_min_shares\", \"self_reference_max_shares\",\n",
        "    \"self_reference_avg_shares\", \"is_weekend\", \"global_subjectivity\",\n",
        "    \"global_sentiment_polarity\", \"global_rate_positive_words\",\n",
        "    \"global_rate_negative_words\", \"rate_positive_words\", \"rate_negative_words\",\n",
        "    \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\",\n",
        "    \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\",\n",
        "    \"title_subjectivity\", \"title_sentiment_polarity\", \"abs_title_subjectivity\",\n",
        "    \"abs_title_sentiment_polarity\"]\n",
        "\n",
        "VOCAB_FEATURE_KEYS = [\"data_channel\"]\n",
        "\n",
        "BUCKET_FEATURE_KEYS = [\"LDA_00\", \"LDA_01\", \"LDA_02\", \"LDA_03\", \"LDA_04\"]\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\"weekday\"]\n",
        "\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [6]\n",
        "\n",
        "#UNUSED: date, slug\n",
        "\n",
        "LABEL_KEY = \"n_shares_percentile\"\n",
        "VOCAB_SIZE = 10\n",
        "OOV_SIZE = 5\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "def transformed_name(key):\n",
        "  return key + '_xf'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH9NP8fYTEPE",
        "colab_type": "text"
      },
      "source": [
        "Now define a module containing the `preprocessing_fn()` function that we will pass to the `Transform` component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3EIuVQnBfH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from online_news_constants import *\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "  for key in DENSE_FLOAT_FEATURE_KEYS:\n",
        "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
        "    outputs[transformed_name(key)] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]))\n",
        "\n",
        "  for key in VOCAB_FEATURE_KEYS:\n",
        "    # Build a vocabulary for this feature.\n",
        "    outputs[transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "        _fill_in_missing(inputs[key]),\n",
        "        top_k=VOCAB_SIZE,\n",
        "        num_oov_buckets=OOV_SIZE)\n",
        "\n",
        "  for key in BUCKET_FEATURE_KEYS:\n",
        "    outputs[transformed_name(key)] = tft.bucketize(\n",
        "        _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT,\n",
        "        always_return_num_quantiles=False)\n",
        "\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[transformed_name(key)] = _fill_in_missing(inputs[key])\n",
        "\n",
        "  # How popular is this article?\n",
        "  outputs[transformed_name(LABEL_KEY)] = _fill_in_missing(inputs[LABEL_KEY])\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bkbpcULTt4p",
        "colab_type": "text"
      },
      "source": [
        "Create a Python module containing a `trainer_fn` function, which must return an estimator.  If you prefer creating a Keras model, you can do so and then convert it to an estimator using `keras.model_to_estimator()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaFFTBBeB4wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from online_news_constants import *\n",
        "\n",
        "\n",
        "def transformed_names(keys):\n",
        "  return [transformed_name(key) for key in keys]\n",
        "\n",
        "\n",
        "# Tf.Transform considers these features as \"raw\"\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "  return tf.data.TFRecordDataset(\n",
        "      filenames,\n",
        "      compression_type='GZIP')\n",
        "\n",
        "\n",
        "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
        "  \"\"\"Build an estimator for predicting the popularity of online news articles\n",
        "\n",
        "  Args:\n",
        "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
        "      estimator (including model_dir).\n",
        "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
        "    warm_start_from: Optional directory to warm start from.\n",
        "\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  real_valued_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=VOCAB_SIZE + OOV_SIZE, default_value=0)\n",
        "      for key in transformed_names(VOCAB_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in transformed_names(BUCKET_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              transformed_names(CATEGORICAL_FEATURE_KEYS),\n",
        "              MAX_CATEGORICAL_FEATURE_VALUES)\n",
        "  ]\n",
        "  return tf.estimator.DNNLinearCombinedRegressor(\n",
        "      config=config,\n",
        "      linear_feature_columns=categorical_columns,\n",
        "      dnn_feature_columns=real_valued_columns,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
        "      warm_start_from=warm_start_from)\n",
        "\n",
        "\n",
        "def _example_serving_receiver_fn(tf_transform_output, schema):\n",
        "  \"\"\"Build the serving in inputs.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "\n",
        "  Returns:\n",
        "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_output.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "      transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "\n",
        "def _eval_input_receiver_fn(tf_transform_output, schema):\n",
        "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "\n",
        "  Returns:\n",
        "    EvalInputReceiver function, which contains:\n",
        "      - Tensorflow graph which parses raw untransformed features, applies the\n",
        "        tf-transform preprocessing operators.\n",
        "      - Set of raw, untransformed features.\n",
        "      - Label against which predictions will be compared.\n",
        "  \"\"\"\n",
        "  # Notice that the inputs are raw features, not transformed features here.\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  features = serving_input_receiver.features.copy()\n",
        "  transformed_features = tf_transform_output.transform_raw_features(features)\n",
        "  \n",
        "  # NOTE: Model is driven by transformed features (since training works on the\n",
        "  # materialized output of TFT, but slicing will happen on raw features.\n",
        "  features.update(transformed_features)\n",
        "\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "      features=features,\n",
        "      receiver_tensors=serving_input_receiver.receiver_tensors,\n",
        "      labels=transformed_features[transformed_name(LABEL_KEY)])\n",
        "\n",
        "\n",
        "def _input_fn(filenames, tf_transform_output, batch_size=200):\n",
        "  \"\"\"Generates features and labels for training or evaluation.\n",
        "\n",
        "  Args:\n",
        "    filenames: [str] list of CSV files to read data from.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: int First dimension size of the Tensors returned by input_fn\n",
        "\n",
        "  Returns:\n",
        "    A (features, indices) tuple where features is a dictionary of\n",
        "      Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
        "\n",
        "  transformed_features = dataset.make_one_shot_iterator().get_next()\n",
        "  # We pop the label because we do not want to use it as a feature while we're\n",
        "  # training.\n",
        "  return transformed_features, transformed_features.pop(\n",
        "      transformed_name(LABEL_KEY))\n",
        "\n",
        "\n",
        "# TFX will call this function\n",
        "def trainer_fn(hparams, schema):\n",
        "  \"\"\"Build the estimator using the high level API.\n",
        "  Args:\n",
        "    hparams: Holds hyperparameters used to train the model as name/value pairs.\n",
        "    schema: Holds the schema of the training examples.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  # Number of nodes in the first layer of the DNN\n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layers = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  train_batch_size = 40\n",
        "  eval_batch_size = 40\n",
        "\n",
        "  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n",
        "\n",
        "  train_input_fn = lambda: _input_fn(\n",
        "      hparams.train_files,\n",
        "      tf_transform_output,\n",
        "      batch_size=train_batch_size)\n",
        "\n",
        "  eval_input_fn = lambda: _input_fn(\n",
        "      hparams.eval_files,\n",
        "      tf_transform_output,\n",
        "      batch_size=eval_batch_size)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      train_input_fn,\n",
        "      max_steps=hparams.train_steps)\n",
        "\n",
        "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  exporter = tf.estimator.FinalExporter('online-news', serving_receiver_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      eval_input_fn,\n",
        "      steps=hparams.eval_steps,\n",
        "      exporters=[exporter],\n",
        "      name='online-news-eval')\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
        "\n",
        "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
        "\n",
        "  estimator = _build_estimator(\n",
        "      # Construct layers sizes with exponetial decay\n",
        "      hidden_units=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layers)\n",
        "      ],\n",
        "      config=run_config,\n",
        "      warm_start_from=hparams.warm_start_from)\n",
        "\n",
        "  # Create an input receiver for TFMA processing\n",
        "  receiver_fn = lambda: _eval_input_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  return {\n",
        "      'estimator': estimator,\n",
        "      'train_spec': train_spec,\n",
        "      'eval_spec': eval_spec,\n",
        "      'eval_input_receiver_fn': receiver_fn\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAika7-6gLvI",
        "colab_type": "text"
      },
      "source": [
        "## Create the Pipeline\n",
        "\n",
        "Creating the pipeline defines the dependencies between components and the artifacts that they require as input, which in turn defines the order in which they can be run.  In this example you also use an ML-Metadata database, which is this case is backed by SQL Lite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNvMj9AWsmSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_pipeline_name = 'online_news_beam'\n",
        "\n",
        "_pipeline_root = tempfile.mkdtemp(prefix='tfx-pipelines')\n",
        "_pipeline_root = os.path.join(_pipeline_root, 'pipelines', _pipeline_name)\n",
        "\n",
        "# Sqlite ML-metadata db path.\n",
        "_metadata_root = tempfile.mkdtemp(prefix='tfx-metadata')\n",
        "_metadata_path = os.path.join(_metadata_root, 'metadata.db')\n",
        "\n",
        "def _create_pipeline(pipeline_name, pipeline_root, data_root,\n",
        "                     transform_module_file, trainer_module_file,\n",
        "                     serving_model_dir, metadata_path):\n",
        "  \"\"\"Implements the online news pipeline with TFX.\"\"\"\n",
        "  input_data = external_input(data_root)\n",
        "\n",
        "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
        "  example_gen = CsvExampleGen(input=input_data)\n",
        "\n",
        "  # Computes statistics over data for visualization and example validation.\n",
        "  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "\n",
        "  # Generates schema based on statistics files.\n",
        "  infer_schema = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'])\n",
        "\n",
        "  # Performs anomaly detection based on statistics and data schema.\n",
        "  validate_stats = ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=infer_schema.outputs['schema'])\n",
        "\n",
        "  # Performs transformations and feature engineering in training and serving.\n",
        "  transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=infer_schema.outputs['schema'],\n",
        "      module_file=transform_module_file)\n",
        "\n",
        "  # Uses user-provided Python function that implements a model using\n",
        "  # TensorFlow's Estimators API.\n",
        "  trainer = Trainer(\n",
        "      module_file=trainer_module_file,\n",
        "      transformed_examples=transform.outputs['transformed_examples'],\n",
        "      schema=infer_schema.outputs['schema'],\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "\n",
        "  # Uses TFMA to compute a evaluation statistics over features of a model.\n",
        "  model_analyzer = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer.outputs['model'],\n",
        "      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n",
        "          evaluator_pb2.SingleSlicingSpec(\n",
        "              column_for_slicing=['weekday'])\n",
        "      ]))\n",
        "\n",
        "  # Performs quality validation of a candidate model (compared to a baseline).\n",
        "  model_validator = ModelValidator(\n",
        "      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])\n",
        "\n",
        "  # Checks whether the model passed the validation steps and pushes the model\n",
        "  # to a file destination if check passed.\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=model_validator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  return pipeline.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      components=[\n",
        "          example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
        "          trainer, model_analyzer, model_validator, pusher\n",
        "      ],\n",
        "      enable_cache=True,\n",
        "      metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "          metadata_path),\n",
        "      additional_pipeline_args={},\n",
        "  )\n",
        "\n",
        "online_news_pipeline = _create_pipeline(\n",
        "        pipeline_name=_pipeline_name,\n",
        "        pipeline_root=_pipeline_root,\n",
        "        data_root=_data_root,\n",
        "        transform_module_file=_transform_module_file,\n",
        "        trainer_module_file=_trainer_module_file,\n",
        "        serving_model_dir=_serving_model_dir,\n",
        "        metadata_path=_metadata_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41nLEd9uW0UW",
        "colab_type": "text"
      },
      "source": [
        "### Run the pipeline\n",
        "\n",
        "Create a `BeamDagRunner` and use it to run the pipeline.\n",
        "\n",
        ">#### Note\n",
        "> This same pattern is also used to create pipelines with other orchestrators, the only difference here being that a Beam orchestrator is used.  When using a Beam orchestrator running the pipeline also triggers an execution run, while other orchestrators may only load the pipeline and wait for a trigger event.\n",
        "\n",
        "#### Results\n",
        "\n",
        "Running this pipeline produces a lot of log messages, which can be instructive to read through.  For example, log messages like these show the sequencing of components through the pipeline.\n",
        "\n",
        "```\n",
        "INFO:tensorflow:Component CsvExampleGen is running.\n",
        "INFO:tensorflow:Run driver for CsvExampleGen\n",
        "...\n",
        "INFO:tensorflow:Run executor for CsvExampleGen\n",
        "...\n",
        "INFO:tensorflow:Run publisher for CsvExampleGen\n",
        "...\n",
        "INFO:tensorflow:Component CsvExampleGen is finished.\n",
        "INFO:tensorflow:Component StatisticsGen is running.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqB2lRBbrvOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BeamDagRunner().run(online_news_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
