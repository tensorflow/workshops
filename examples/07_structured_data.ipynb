{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Structured Data\n",
    "\n",
    "In this notebook, we'll train models on structued data (e.g., a CSV file) using the new [Datasets API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md) and Estimators. Together, these give you a way to efficiently train neural networks on potentially large amounts of data (e.g., more than could fit into memory). We'll demonstrate a number of different techniques you can use to represent your features, including bucketing, embeddings, and so forth. That's the interesting part of this notebook, and it's worth experimenting with.\n",
    "\n",
    "### Neat optional bonus\n",
    "\n",
    "You can explore this dataset with [Facets](https://github.com/pair-code/facets) - try the [online demo](https://pair-code.github.io/facets/) or optionally download and install.\n",
    "\n",
    "### Before getting started\n",
    "\n",
    "* This code requires TensorFlow ***version 1.3+*** (which at the time of writing, has not been released). You can install the pre-release, v1.3rc2. See the [README](../README.md) for instructions.\n",
    "\n",
    "### Tip\n",
    "\n",
    "Finally, a last tip if you run this code multiple times. The estimators will automatically load and continue training from checkpoints, so if you modify the features and would like to start from scratch, be sure to delete ```./census``` (the saved models directory) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print('This code requires TensorFlow v1.3+')\n",
    "print('You have:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "Here, we'll work with the [Adult dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/old.adult.names). Our task is to predict whether a given adult makes more than $50,000 a year, based attributes such as their occupation, and the number of hours they work per week. The code here presented can become a starting point for a problem you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]\n",
    "\n",
    "census_train = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "census_test = pd.read_csv(census_test_path, index_col=False, names=column_names) \n",
    "\n",
    "# Convert the label column to true/false\n",
    "census_train_label = census_train.pop('income') == \" >50K\" \n",
    "census_test_label = census_test.pop('income') == \" >50K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (\"Training examples: %d\" % census_train.shape[0])\n",
    "print (\"Test examples: %d\" % census_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# handy method to preview the data\n",
    "census_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here's how the label looks after we've converted it\n",
    "census_train_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to train a linear model\n",
    "We'll train a logistic regression model to start, then we'll use a DNN. There are different considerations in how you represent your features for linear and deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Datasets API to write an Input Function\n",
    "\n",
    "We'll use the same input functions for both the linear and deep model. Note, we could also have used the [pandas input function](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn), but the Datasets API has a nicer interface and scales better - you should use Datasets moving forward.\n",
    "\n",
    "There's some additional code below to handle small quirks in the format of the CSV files we're using, see the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify default values for each of the CSV columns.\n",
    "csv_defaults = collections.OrderedDict([\n",
    "  ('age',[0]),\n",
    "  ('workclass',['']),\n",
    "  ('fnlwgt',[0]),\n",
    "  ('education',['']),\n",
    "  ('education-num',[0]),\n",
    "  ('marital-status',['']),\n",
    "  ('occupation',['']),\n",
    "  ('relationship',['']),\n",
    "  ('race',['']),\n",
    "  ('sex',['']),\n",
    "  ('capital-gain',[0]),\n",
    "  ('capital-loss',[0]),\n",
    "  ('hours-per-week',[0]),\n",
    "  ('native-country',['']),\n",
    "  ('income',['']),\n",
    "])\n",
    "\n",
    "# Here's how we'll decode each line\n",
    "def csv_decoder(line):\n",
    "    \"\"\"Converts a CSV row to a dictonary containing each feature.\"\"\"\n",
    "    #print(tf.size(line))\n",
    "    parsed = tf.decode_csv(line, list(csv_defaults.values()))\n",
    "    return dict(zip(csv_defaults.keys(), parsed))\n",
    "\n",
    "# The train file has an extra empty line at the end.\n",
    "# We want to ignore this.\n",
    "def filter_empty_lines(line):\n",
    "    \"\"\"Returns true if the line is empty and False otherwise.\"\"\"\n",
    "    return tf.not_equal(tf.size(tf.string_split([line], ',').values), 0)\n",
    "\n",
    "def train_input_fn(path):\n",
    "    def input_fn():    \n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(path)  # create dataset from disk file\n",
    "                .filter(filter_empty_lines)  # ignore empty lines\n",
    "                .map(csv_decoder)  # get values on the csv row\n",
    "                .shuffle(buffer_size=1000)  # shuffle the dataset, careful with the buffer_size\n",
    "                .repeat()  # repeate the dataset indefinitely\n",
    "                .batch(32))  # batch the data\n",
    "\n",
    "        # create iterator\n",
    "        columns = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        # separate the label and convert it to true/false\n",
    "        income = tf.equal(columns.pop('income'),\" >50K\") \n",
    "        return columns, income\n",
    "    return input_fn\n",
    "\n",
    "def test_input_fn(path):\n",
    "    def input_fn():    \n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(path)  # create dataset from disk file\n",
    "                .skip(1) # the test file has a strange first line, we want to ignore this.\n",
    "                .filter(filter_empty_lines)  # ignore empty lines\n",
    "                .map(csv_decoder)  # get values on the csv row\n",
    "                .batch(32))  # batch the data\n",
    "\n",
    "        # create iterator\n",
    "        columns = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        # separate the label and convert it to true/false\n",
    "        income = tf.equal(columns.pop('income'),\" >50K\") \n",
    "        return columns, income\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the features we'll use and how we'd like them represented\n",
    "\n",
    "Our goal here is to demostrate how to work with different types of features, rather than to aim for an accurate model. Here are five different types we'll use:\n",
    "* A *numeric_column*. This is just a real-valued attribute.\n",
    "\n",
    "\n",
    "* A *bucketized_column*. TensorFlow automatically buckets a numeric column for us.\n",
    "\n",
    "\n",
    "* A *categorical_column_with_vocabulary_list*. This is just a categorical column, where you know the possible values in advance. This is useful when you have a small number of possibilities.\n",
    "\n",
    "\n",
    "* A *categorical_column_with_hash_bucket*. This is a useful way to represent categorical features when you have a large number of values. Beware of hash collisions.\n",
    "\n",
    "\n",
    "* A *crossed_column*. Linear models cannot consider interactions between features, so we'll ask TensorFlow to cross features for us.\n",
    "\n",
    "Using these can be nicer than having to manually preprocess your data in many different ways as you experiment. You can see a more extensive example of them in action [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/learn/wide_n_deep_tutorial.py) (note: this code hasn't yet been updated for v1.3), and you can read more about feature columns [here](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/feature_column) and [here](https://www.tensorflow.org/tutorials/wide_and_deep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a numeric feature\n",
    "hour_per_week = tf.feature_column.numeric_column('hours-per-week')\n",
    "\n",
    "# a bucketed feature\n",
    "# you can also specify the bucket values if you prefer\n",
    "# here, as below\n",
    "education_num = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('education-num'), \n",
    "    list(range(10))\n",
    ")\n",
    "\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('age'), \n",
    "    boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    ")\n",
    "\n",
    "# a categorical feature with a known list of values\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list('sex', ['male','female'])\n",
    "\n",
    "# a categorical feature with a possibly large number of values\n",
    "# beware of hash collisions\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000)\n",
    "\n",
    "# a crossed column\n",
    "education_num_x_gender = tf.feature_column.crossed_column(\n",
    "    [education_num, gender],\n",
    "    hash_bucket_size=int(1e4)\n",
    ")\n",
    "\n",
    "# these are the features we'll use for our linear model\n",
    "feature_columns = [\n",
    "    hour_per_week,\n",
    "    education_num,\n",
    "    age_buckets,\n",
    "    gender,\n",
    "    native_country,\n",
    "    education_num_x_gender\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = train_input_fn(census_train_path)\n",
    "test_input = test_input_fn(census_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two block, I thought I'd add some code you can use to debug your input functions. It won't normally be necessary to create a Session and run them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_batch = train_input()\n",
    "with tf.Session() as sess:\n",
    "    features, label = sess.run(training_batch)\n",
    "    print(features['education'])\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_batch = test_input()\n",
    "with tf.Session() as sess:\n",
    "    features, label = sess.run(testing_batch)\n",
    "    print(features['education'])\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate a Canned Linear Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearClassifier(feature_columns, model_dir='census/linear',n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.train(train_input, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an embedding feature(!)\n",
    "\n",
    "Instead of using a hashbucket to represent categorical features, why not use a learned embedding. (Cool, right?) We'll also update how the features are represented for our deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column('education-num'),\n",
    "    tf.feature_column.numeric_column('hours-per-week'),\n",
    "    tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('age'), \n",
    "        boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
    "    ),\n",
    "    tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list('sex',['male','female'])),\n",
    "    tf.feature_column.embedding_column(  # now using embedding!\n",
    "        tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000), 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restart the input functions\n",
    "train_input = train_input_fn(census_train_path)\n",
    "test_input = test_input_fn(census_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a deep model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNClassifier(hidden_units=[128,128], \n",
    "                                       feature_columns=feature_columns, \n",
    "                                       n_classes=2, \n",
    "                                       model_dir='census/dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.train(train_input, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with features and params!\n",
    "# If you'd like to learn how to train a joint model,\n",
    "# check out the wide and deep tutorial above.\n",
    "estimator.evaluate(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
