{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX Lab 2 – Introduction to Apache Beam",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "R_vFbXN2T7oo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vFbXN2T7oo",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright &copy; 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhL09apzT-W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW",
        "colab_type": "text"
      },
      "source": [
        "# TFX – Introduction to Apache Beam\n",
        "\n",
        "TFX is designed to be scalable to very large datasets which require substantial resources.  Distributed pipeline frameworks such as Apache Beam offer the ability to distribute processing across compute clusters and apply the resources required.  Many of the standard TFX components use Apache Beam, and custom components that you may write may also benefit from using Apache Beam for distibuted processing.\n",
        "\n",
        "This notebook introduces the concepts and code patterns for developing with the Apache Beam Python API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "First, we install the necessary packages, download data, import modules and set up paths.\n",
        "\n",
        "### Install TensorFlow and Apache Beam\n",
        "\n",
        "> #### Note\n",
        "> Because of some of the updates to packages you must use the button at the bottom of the output of this cell to restart the runtime.  Following restart, you should rerun this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zhcJBLoMXQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U \\\n",
        "  tensorflow==2.0.0 \\\n",
        "  apache-beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q",
        "colab_type": "text"
      },
      "source": [
        "### Import packages\n",
        "We import necessary packages, including Beam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam import pvalue\n",
        "from apache_beam.runners.interactive.display import pipeline_graph\n",
        "import graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0uuFfIiHviyW",
        "colab": {}
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('Beam version: {}'.format(beam.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAika7-6gLvI",
        "colab_type": "text"
      },
      "source": [
        "## Create a Beam Pipeline\n",
        "\n",
        "Create a pipeline, including a simple [`PCollection`](https://beam.apache.org/releases/javadoc/2.1.0/org/apache/beam/sdk/values/PCollection.html) and a [`ParDo()`](https://beam.apache.org/releases/javadoc/2.0.0/org/apache/beam/sdk/transforms/ParDo.html) transform.\n",
        "\n",
        "* A `PCollection<T>` is an **immutable collection** of values of type `T`. A `PCollection` can contain either a bounded or unbounded number of elements. Bounded and unbounded `PCollections` are produced as the output of `PTransforms` (including root `PTransforms` like `Read` and `Create`), and can be passed as the inputs of other `PTransforms`.\n",
        "* `ParDo` is the core element-wise transform in Apache Beam, invoking a user-specified function on each of the elements of the input `PCollection` to produce zero or more output elements, all of which are collected into the output `PCollection`.\n",
        "\n",
        "First, use the `.run()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_XV_cm2_E1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_pipeline = beam.Pipeline()\n",
        "\n",
        "lines = (first_pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello\", \"World\", \"!!!\"]) # PCollection\n",
        "         | \"Print\" >> beam.ParDo(print)) # ParDo transform\n",
        "\n",
        "result = first_pipeline.run()\n",
        "result.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx1xFZ-BYccz",
        "colab_type": "text"
      },
      "source": [
        "Display the structure of this pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdnEk2p3Jnfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_pipeline(pipeline):\n",
        "  graph = pipeline_graph.PipelineGraph(pipeline)\n",
        "  return graphviz.Source(graph.get_dot())\n",
        "\n",
        "display_pipeline(first_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSaFbVyIYNIA",
        "colab_type": "text"
      },
      "source": [
        "Next, invoke run inside a `with` block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4O8EAYNULbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as with_pipeline:\n",
        "  lines = (with_pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello\", \"World\", \"!!!\"])\n",
        "         | \"Print\" >> beam.ParDo(print))\n",
        "\n",
        "display_pipeline(with_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMYiWcBtIBF7",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1 — Creating and Running Your Beam Pipeline**\n",
        "\n",
        "1. Build a Beam pipeline that creates a PCollection containing integers 0 to 10 and prints them.\n",
        "2. Add a step in the pipeline to square each item.\n",
        "3. Display the pipeline.\n",
        "\n",
        "*Warning*: the `ParDo()` method must either return `None` or a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aPlwJjQKmo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_Nf9XkH-vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W46JR_uDH-lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8NJsj-K1hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai6SmbkuK1SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-b7ZdFYLoH2",
        "colab_type": "text"
      },
      "source": [
        "![Solution](https://www.tensorflow.org/site-assets/images/marketing/learn/tfx-hero.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbw50WdgMsLU",
        "colab_type": "text"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSUBzUGqfX4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as with_pipeline:\n",
        "  lines = (with_pipeline\n",
        "         | \"Create\" >> beam.Create(range(10 + 1))\n",
        "         | \"Square\" >> beam.ParDo(lambda x: [x ** 2])\n",
        "         | \"Print\" >> beam.ParDo(print))\n",
        "\n",
        "display_pipeline(with_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6lzLoIQz1Oe",
        "colab_type": "text"
      },
      "source": [
        "# Core Transforms\n",
        "---\n",
        "Beam has a set of core transforms on data that is contained in PCollections.  In the cells that follow, explore several core transforms and observe the results in order to develop some understanding and intuition for what each transform does.\n",
        "\n",
        "## [Map](https://beam.apache.org/documentation/transforms/python/elementwise/map/)\n",
        "\n",
        "The `Map` transform applies a simple 1-to-1 mapping function over each element in the collection.  `Map` accepts a function that returns a single element for every input element in the `PCollection`.  You can pass functions with multiple arguments to `Map`. They are passed as additional positional arguments or keyword arguments to the function.\n",
        "\n",
        "First, compare the results of a `ParDo` transform and a `Map` transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkzO7gANza49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | \"Create\" >> beam.Create([1, 2, 3])\n",
        "           | \"Multiply\" >> beam.ParDo(lambda number: [number * 2]) # ParDo with integers\n",
        "           | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZWhKgqn02Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "         | \"Create\" >> beam.Create([1, 2, 3])\n",
        "         | \"Multiply\" >> beam.Map(lambda number: number * 2) # Map with integers\n",
        "         | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bsEdlKj0Wce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello Beam\", \"This is cool\"])\n",
        "         | \"Split\" >> beam.ParDo(lambda sentence: sentence.split()) # ParDo with strings\n",
        "         | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY490-duaQFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello Beam\", \"This is cool\"])\n",
        "         | \"Split\" >> beam.Map(lambda sentence: sentence.split()) # Map with strings\n",
        "         | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-gNKm70oJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BreakIntoWordsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        return element.split()\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello Beam\", \"This is cool\"])\n",
        "         | \"Split\" >> beam.ParDo(BreakIntoWordsDoFn()) # Apply a DoFn with a process method\n",
        "         | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99n6mPVF1C1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "         | \"Create\" >> beam.Create([\"Hello Beam\", \"This is cool\"])\n",
        "         | \"Split\" >> beam.FlatMap(lambda sentence: sentence.split()) # Compare to a FlatMap\n",
        "         | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbthg5uU1X4a",
        "colab_type": "text"
      },
      "source": [
        "## [GroupByKey](https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey/)\n",
        "\n",
        "`GroupByKey` takes a keyed collection of elements and produces a collection where each element consists of a key and all values associated with that key.\n",
        "\n",
        "`GroupByKey` is a transform for processing collections of key/value pairs. It’s a parallel reduction operation, analogous to the Shuffle phase of a Map/Shuffle/Reduce-style algorithm. The input to `GroupByKey` is a collection of key/value pairs that represents a multimap, where the collection contains multiple pairs that have the same key, but different values. Given such a collection, you use `GroupByKey` to collect all of the values associated with each unique key.\n",
        "\n",
        "`GroupByKey` is a good way to aggregate data that has something in common. For example, if you have a collection that stores records of customer orders, you might want to group together all the orders from the same postal code (wherein the “key” of the key/value pair is the postal code field, and the “value” is the remainder of the record)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTX7JoS11LQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(['apple', 'ball', 'car', 'bear', 'cheetah', 'ant'])\n",
        "           | beam.Map(lambda word: (word[0], word))\n",
        "           | beam.GroupByKey()\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DKNvm2H7hrsw"
      },
      "source": [
        "**Exercise 2 — Group Items by Key**\n",
        "\n",
        "1. Build a Beam pipeline that creates a PCollection containing integers 0 to 10 and prints them.\n",
        "2. Add a step in the pipeline to add a key to each item that will indicate whether it is even or odd.\n",
        "3. Use `GroupByKey` to group even items together and odd items together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KwinUX5-hrs0",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9rm6a2W7hrs3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ituEBJzhrs5",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqjIVuc1hrs8",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BFW9cwPMhrtA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OkrvfvXohrtB"
      },
      "source": [
        "![Solution](https://www.tensorflow.org/site-assets/images/marketing/learn/tfx-hero.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "euNLIh_ThrtC"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3_dXXXIiM1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(10 + 1))\n",
        "           | beam.Map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
        "           | beam.GroupByKey()\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsFACrDYgZ9-",
        "colab_type": "text"
      },
      "source": [
        "`CoGroupByKey` can combine multiple PCollections, assuming every element is a tuple whose first item is the key to join on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Mefr2d1fJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = beam.Pipeline()\n",
        "\n",
        "fruits = pipeline | 'Fruits' >> beam.Create(['apple',\n",
        "                                             'banana',\n",
        "                                             'cherry'])\n",
        "countries = pipeline | 'Countries' >> beam.Create(['australia',\n",
        "                                                   'brazil',\n",
        "                                                   'belgium',\n",
        "                                                   'canada'])\n",
        "\n",
        "def add_key(word):\n",
        "    return (word[0], word)\n",
        "\n",
        "fruits_with_keys = fruits | \"fruits_with_keys\" >> beam.Map(add_key)\n",
        "countries_with_keys = countries | \"countries_with_keys\" >> beam.Map(add_key)\n",
        "\n",
        "({\"fruits\": fruits_with_keys, \"countries\": countries_with_keys}\n",
        "    | beam.CoGroupByKey()\n",
        "    | beam.Map(print))\n",
        "\n",
        "pipeline.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPi2bXuh5a9l",
        "colab_type": "text"
      },
      "source": [
        "## [Combine](https://beam.apache.org/documentation/programming-guide/#combine)\n",
        "\n",
        "`Combine` is a transform for combining collections of elements or values. `Combine` has variants that work on entire `PCollections`, and some that combine the values for each key in `PCollections` of key/value pairs.\n",
        "\n",
        "To apply a `Combine transform`, you must provide the function that contains the logic for combining the elements or values. The combining function should be commutative and associative, as the function is not necessarily invoked exactly once on all values with a given key. Because the input data (including the value collection) **may be distributed across multiple workers**, the combining function might be called multiple times to perform partial combining on subsets of the value collection. The Beam SDK also provides some pre-built combine functions for common numeric combination operations such as `sum`, `min`, and `max`.\n",
        "\n",
        "Simple combine operations, such as sums, can usually be implemented as a simple function. More complex combination operations might require you to create a subclass of `CombineFn` that has an accumulation type distinct from the input/output type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWC8ywHC5KQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create([1, 2, 3, 4, 5])\n",
        "           | beam.CombineGlobally(sum)\n",
        "           | beam.Map(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS58V5cd5pQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create([1, 2, 3, 4, 5])\n",
        "           | beam.combiners.Mean.Globally()\n",
        "           | beam.Map(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R9zt-xX5h-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageFn(beam.CombineFn):\n",
        "    def create_accumulator(self):\n",
        "        return (0.0, 0)\n",
        "\n",
        "    def add_input(self, accumulator, input_):\n",
        "        total, count = accumulator\n",
        "        total += input_\n",
        "        count += 1\n",
        "        return (total, count)\n",
        "\n",
        "    def merge_accumulators(self, accumulators):\n",
        "        totals, counts = zip(*accumulators)\n",
        "        return sum(totals), sum(counts)\n",
        "\n",
        "    def extract_output(self, accumulator):\n",
        "        total, count = accumulator\n",
        "        return total / count if count else float(\"NaN\")\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create([1, 2, 3, 4, 5])\n",
        "           | beam.CombineGlobally(AverageFn())\n",
        "           | beam.Map(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N0So44g6_8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(['bob', 'alice', 'alice', 'bob', 'charlie', 'alice'])\n",
        "           | beam.combiners.Count.PerElement()\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vafplz7d56CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(['bob', 'alice', 'alice', 'bob', 'charlie', 'alice'])\n",
        "           | beam.Map(lambda word: (word, 1))\n",
        "           | beam.CombinePerKey(sum)\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lswZZCAe6Th5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(['bob', 'alice', 'alice', 'bob', 'charlie', 'alice'])\n",
        "           | beam.combiners.Count.Globally()\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgoLTv8gi0WW"
      },
      "source": [
        "**Exercise 3 — Combine Items**\n",
        "\n",
        "1. Start with Beam pipeline you built in the previous exercise: it creates a PCollection containing integers 0 to 10, groups them by their parity, and prints the groups.\n",
        "2. Add a step that computes the mean of each group (i.e., the mean of all odd numbers between 0 and 10, and the mean of all even numbers between 0 and 10).\n",
        "3. Add another step to make the pipeline compute the mean of the squares of the numbers in each group.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cjw3Sqjci0Wd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AalDelXSi0Wh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EPb9-f9Ri0Wj",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LQ3EL2QFi0Wl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pgbf6O42i0Wo",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6SzSYQ2ei0Wp"
      },
      "source": [
        "![Solution](https://www.tensorflow.org/site-assets/images/marketing/learn/tfx-hero.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMYzcG46i0Wq"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOWJXA_jt1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(10 + 1))\n",
        "           | beam.Map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
        "           | beam.Map(lambda x: (x[0], x[1] ** 2))\n",
        "           | beam.CombinePerKey(AverageFn())\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-SEoKnd8cea",
        "colab_type": "text"
      },
      "source": [
        "## [Flatten](https://beam.apache.org/documentation/programming-guide/#flatten)\n",
        "\n",
        "`Flatten` is a transform for `PCollection` objects that store **the same data type**. `Flatten` merges multiple `PCollection` objects into a single logical `PCollection`.\n",
        "\n",
        "#### Data encoding in merged collections\n",
        "By default, the coder for the output `PCollection` is the same as the coder for the first `PCollection` in the input `PCollectionList`. However, the input `PCollection` objects can each use different coders, **as long as they all contain the same data type** in your chosen language.\n",
        "\n",
        "#### Merging windowed collections\n",
        "When using `Flatten` to merge `PCollection` objects that have a windowing strategy applied, all of the `PCollection` objects you want to merge must use a compatible windowing strategy and window sizing. For example, all the collections you're merging must all use (hypothetically) identical 5-minute fixed windows or 4-minute sliding windows starting every 30 seconds.\n",
        "\n",
        "If your pipeline attempts to use `Flatten` to merge `PCollection` objects with incompatible windows, Beam generates an `IllegalStateException` error when your pipeline is constructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BVhknBM7Frz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = beam.Pipeline()\n",
        "\n",
        "wordsStartingWithA = (pipeline\n",
        "  | 'Words starting with A' >> beam.Create(['apple', 'ant', 'arrow']))\n",
        "\n",
        "wordsStartingWithB = (pipeline\n",
        "  | 'Words starting with B' >> beam.Create(['ball', 'book', 'bow']))\n",
        "\n",
        "((wordsStartingWithA, wordsStartingWithB)\n",
        "    | beam.Flatten()\n",
        "    | beam.ParDo(print))\n",
        "\n",
        "pipeline.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvaC0Zvj9Cse",
        "colab_type": "text"
      },
      "source": [
        "## [Partition](https://beam.apache.org/documentation/programming-guide/#partition)\n",
        "\n",
        "`Partition` is a transform for `PCollection` objects that store **the same data type**. `Partition` splits a single `PCollection` into a fixed number of smaller collections.\n",
        "\n",
        "`Partition` divides the elements of a `PCollection` according to a **partitioning function** that you provide. The partitioning function contains the logic that determines how to split up the elements of the input `PCollection` into each resulting partition `PCollection`. The number of partitions must be determined at graph construction time. You can, for example, pass the number of partitions as a command-line option at runtime (which will then be used to build your pipeline graph), but you cannot determine the number of partitions in mid-pipeline (based on data calculated after your pipeline graph is constructed, for instance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6wjB7Y48olW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition_fn(number, num_partitions):\n",
        "    partition = number // 100\n",
        "    return min(partition, num_partitions - 1)\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create([1, 110, 2, 350, 4, 5, 100, 150, 3])\n",
        "           | beam.Partition(partition_fn, 3))\n",
        "\n",
        "  lines[0] | '< 100' >> beam.ParDo(print, \"Small\")\n",
        "  lines[1] | '[100, 200)' >> beam.ParDo(print, \"Medium\")\n",
        "  lines[2] | '> 200' >> beam.ParDo(print, \"Big\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBiNHmlcFBb1",
        "colab_type": "text"
      },
      "source": [
        "## [Side Inputs](https://beam.apache.org/documentation/programming-guide/#side-inputs)\n",
        "\n",
        "In addition to the main input `PCollection`, you can provide additional inputs to a `ParDo` transform in the form of side inputs. A side input is an additional input that your `DoFn` can access each time it processes an element in the input `PCollection`. When you specify a side input, you create a view of some other data that can be read from within the `ParDo` transform’s `DoFn` while processing each element.\n",
        "\n",
        "Side inputs are useful if your `ParDo` needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime (and not hard-coded). Such values might be determined by the input data, or depend on a different branch of your pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTZWcmdB9eB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def increment(number, inc=1):\n",
        "    return number + inc\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | \"Create\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "           | \"Increment\" >> beam.Map(increment)\n",
        "           | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Men6SXjGFkKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | \"Create\" >> beam.Create([1, 2, 3, 4, 5])\n",
        "           | \"Increment\" >> beam.Map(increment, 10) # Pass a side input of 10\n",
        "           | \"Print\" >> beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2LAoh1fupfp",
        "colab_type": "text"
      },
      "source": [
        "## [Additional Outputs](https://beam.apache.org/documentation/programming-guide/#additional-outputs)\n",
        "\n",
        "While `ParDo` always produces a main output `PCollection` (as the return value from `apply`), you can also have your `ParDo` produce any number of additional output `PCollections`. If you choose to have multiple outputs, your `ParDo` returns all of the output `PCollections` (including the main output) bundled together.\n",
        "\n",
        "To emit elements to multiple output `PCollections`, invoke `with_outputs()` on the `ParDo`, and specify the expected tags for the outputs. `with_outputs()` returns a `DoOutputsTuple` object. Tags specified in `with_outputs` are attributes on the returned `DoOutputsTuple` object. The tags give access to the corresponding output `PCollections`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlv1qnPeFt54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute(number):\n",
        "  if number % 2 == 0:\n",
        "      yield number\n",
        "  else:\n",
        "      yield pvalue.TaggedOutput(\"odd\", number + 10)\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  even, odd = (pipeline\n",
        "           | \"Create\" >> beam.Create([1, 2, 3, 4, 5, 6, 7])\n",
        "           | \"Increment\" >> beam.ParDo(compute).with_outputs(\"odd\",\n",
        "                                                             main=\"even\"))\n",
        "  even | \"Even\" >> beam.ParDo(print, \"even\")\n",
        "  odd | \"Odd\" >> beam.ParDo(print, \"odd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFrovl8IGsB",
        "colab_type": "text"
      },
      "source": [
        "## [Branching](https://beam.apache.org/documentation/programming-guide/#applying-transforms)\n",
        "\n",
        "A transform does not consume or otherwise alter the input collection – remember that a `PCollection` is immutable by definition. This means that you can apply multiple transforms to the same input `PCollection` to create a branching pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9hrASXGmug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as branching_pipeline:\n",
        "  numbers = (branching_pipeline | beam.Create([1, 2, 3, 4, 5]))\n",
        "\n",
        "  mult5_results = numbers | beam.Map(lambda num: num * 5)\n",
        "  mult10_results = numbers | beam.Map(lambda num: num * 10)\n",
        "\n",
        "  mult5_results | 'Log multiply 5' >> beam.ParDo(print, 'Mult 5')\n",
        "  mult10_results | 'Log multiply 10' >> beam.ParDo(print, 'Mult 10')\n",
        "\n",
        "display_pipeline(branching_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-_FQ3S9IotY",
        "colab_type": "text"
      },
      "source": [
        "## [Composite Transforms](https://beam.apache.org/documentation/programming-guide/#composite-transforms)\n",
        "\n",
        "Transforms can have a nested structure, where a complex transform performs multiple simpler transforms (such as more than one `ParDo`, `Combine`, `GroupByKey`, or even other composite transforms). These transforms are called ***composite transforms***. Nesting multiple transforms inside a single composite transform can make your code more modular and easier to understand.\n",
        "\n",
        "Your composite transform's parameters and return value must match the initial input type and final return type for the entire transform, even if the transform's intermediate data changes type multiple times.\n",
        "\n",
        "To create a composite transform, create a subclass of the `PTransform` class and override the `expand` method to specify the actual processing logic. Then use this transform just as you would a built-in transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8GplhSxIY8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtractAndMultiplyNumbers(beam.PTransform):\n",
        "    def expand(self, pcollection):\n",
        "        return (pcollection\n",
        "            | beam.FlatMap(lambda line: line.split(\",\"))\n",
        "            | beam.Map(lambda num: int(num) * 10))\n",
        "\n",
        "with beam.Pipeline() as composite_pipeline:\n",
        "  lines = (composite_pipeline\n",
        "           | beam.Create(['1,2,3,4,5', '6,7,8,9,10'])\n",
        "           | ExtractAndMultiplyNumbers()\n",
        "           | beam.ParDo(print))\n",
        "\n",
        "display_pipeline(composite_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7NFpWUMJIJy",
        "colab_type": "text"
      },
      "source": [
        "## [Filter](https://beam.apache.org/documentation/transforms/python/elementwise/filter/)\n",
        "\n",
        "`Filter`, given a predicate, filters out all elements that don't satisfy that predicate. `Filter` may also be used to filter based on an inequality with a given value based on the comparison ordering of the element.  You can pass functions with multiple arguments to `Filter`. They are passed as additional positional arguments or keyword arguments to the function.  If the `PCollection` has a single value, such as the average from another computation, passing the `PCollection` as a *singleton* accesses that value.  If the `PCollection` has multiple values, pass the `PCollection` as an *iterator*. This accesses elements lazily as they are needed, so it is possible to iterate over large `PCollections` that won't fit into memory.\n",
        "\n",
        "> Note: You can pass the `PCollection` as a list with `beam.pvalue.AsList(pcollection)`, but this requires that all the elements fit into memory.\n",
        "\n",
        "If a `PCollection` is small enough to fit into memory, then that `PCollection` can be passed as a dictionary. Each element must be a (key, value) pair. Note that all the elements of the `PCollection` must fit into memory. If the `PCollection` won't fit into memory, use `beam.pvalue.AsIter(pcollection)` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFN36jVI4gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FilterOddNumbers(beam.DoFn):\n",
        "    def process(self, element, *args, **kwargs):\n",
        "        if element % 2 == 1:\n",
        "            yield element\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.ParDo(FilterOddNumbers())\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQlSBJWJYrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.Filter(lambda num: num % 2 == 1)\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7kHP5sJphJ",
        "colab_type": "text"
      },
      "source": [
        "## [Aggregation](https://beam.apache.org/documentation/programming-guide/)\n",
        "\n",
        "Beam uses [windowing](https://beam.apache.org/documentation/programming-guide/#windowing) to divide a continuously updating unbounded `PCollection` into logical windows of finite size. These logical windows are determined by some characteristic associated with a data element, such as a timestamp. [Aggregation transforms](https://beam.apache.org/documentation/transforms/python/overview/#aggregation) (such as GroupByKey and Combine) work on a per-window basis — as the data set is generated, they process each `PCollection` as a succession of these finite windows.\n",
        "\n",
        "A related concept, called [triggers](https://beam.apache.org/documentation/programming-guide/#triggers), determines when to emit the results of aggregation as unbounded data arrives. You can use triggers to refine the windowing strategy for your `PCollection`. Triggers allow you to deal with late-arriving data, or to provide early results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3W-h0AzJg3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.combiners.Count.Globally() # Count\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZVWg2AvKxUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.CombineGlobally(sum) # CombineGlobally sum\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpkdeF6YK2gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.combiners.Mean.Globally() # Mean\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYJCGAgdK227",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.combiners.Top.Smallest(1) # Top Smallest\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXySuL7QK4sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.Create(range(1, 11))\n",
        "           | beam.combiners.Top.Largest(1) # Top Largest\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMKY3LCWLGXu",
        "colab_type": "text"
      },
      "source": [
        "# [Pipeline I/O](https://beam.apache.org/documentation/programming-guide/#pipeline-io)\n",
        "\n",
        "When you create a pipeline, you often need to read data from some external source, such as a file or a database. Likewise, you may want your pipeline to output its result data to an external storage system. Beam provides read and write transforms for a [number of common data storage types](https://beam.apache.org/documentation/io/built-in/). If you want your pipeline to read from or write to a data storage format that isn’t supported by the built-in transforms, you can [implement your own read and write transforms](https://beam.apache.org/documentation/io/developing-io-overview/).\n",
        "\n",
        "### Download example data\n",
        "Download the sample dataset for use with the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywX6OUEhAqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'https://raw.githubusercontent.com/ageron/open-datasets/master/' \\\n",
        "   'online_news_popularity_for_course/online_news_popularity_for_course.csv'\n",
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqn4wST2Bex5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head {_data_filepath}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz4oEI90mB-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.io.ReadFromText(_data_filepath)\n",
        "           | beam.Filter(lambda line: line.startswith(\"2013-01-07,0,World\"))\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8siWql-89oq1",
        "colab_type": "text"
      },
      "source": [
        "### Putting Everything Together\n",
        "\n",
        "Use several of the concepts, classes, and methods discussed above in a concrete example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RELnubTHl6sK"
      },
      "source": [
        "**Exercise 4 — Reading, Filtering, Parsing, Grouping and Averaging**\n",
        "\n",
        "Write a Beam pipeline that reads the dataset, computes the mean label (the numbers in the last column) for each article category (the third column) and prints out the results.\n",
        "\n",
        "*Hints*:\n",
        "* Use the code above to read the dataset and change the filtering logic to keep only the year 2013.\n",
        "* Add a `Map` step to split each row on the commas.\n",
        "* Add another `Map` step to add a key equal to the category, and a `GroupByKey` step to group the articles by their category.\n",
        "* Add a step to convert the last column (i.e., the label) to a float, and another step to compute the mean of that column for each category, using `beam.combiners.Mean.PerKey`.\n",
        "* Finally, add a `ParDo` step to print out the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I_jeg125l6sR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3q6OTO5Xl6sU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U5sVJaojl6sW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y7LfH_rHl6sZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSVxnmnl6sa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8pjA5uFl6sc"
      },
      "source": [
        "![Solution](https://www.tensorflow.org/site-assets/images/marketing/learn/tfx-hero.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I2mfIjMbl6sc"
      },
      "source": [
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGLY9QRJLB8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as pipeline:\n",
        "  lines = (pipeline\n",
        "           | beam.io.ReadFromText(_data_filepath)\n",
        "           | beam.Filter(lambda line: line < \"2014-01-01\")\n",
        "           | beam.Map(lambda line: line.split(\",\")) # CSV parser?\n",
        "           | beam.Map(lambda cols: (cols[2], float(cols[-1])))\n",
        "           | beam.combiners.Mean.PerKey()\n",
        "           | beam.ParDo(print))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGEzjUGaRHCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.io.TFRecordWriter(\"test.tfrecord\") as tfrecord_file:\n",
        "  for index in range(10):\n",
        "    tfrecord_file.write(\"Record {}\".format(index).encode(\"utf-8\"))\n",
        "\n",
        "dataset = tf.data.TFRecordDataset('test.tfrecord')\n",
        "for record in dataset:\n",
        "  print(record.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgRAt4nRR-qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as rw_pipeline:\n",
        "  lines = (rw_pipeline\n",
        "           | beam.io.ReadFromTFRecord(\"test.tfrecord\")\n",
        "           | beam.Map(lambda line: line + b' processed')\n",
        "           | beam.io.WriteToTFRecord(\"test_processed.tfrecord\")\n",
        "           | beam.ParDo(print))\n",
        "\n",
        "display_pipeline(rw_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT_bo1VxTbTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with beam.Pipeline() as utf_pipeline:\n",
        "  lines = (utf_pipeline\n",
        "           | \"Read\" >> beam.io.ReadFromTFRecord(\"test_processed.tfrecord-00000-of-00001\")\n",
        "           | \"Decode\" >> beam.Map(lambda line: line.decode('utf-8'))\n",
        "           | \"Print\" >> beam.ParDo(print))\n",
        "\n",
        "display_pipeline(utf_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCAysNHLQK80",
        "colab_type": "text"
      },
      "source": [
        "Note that there are many [other built-in I/O transforms](https://beam.apache.org/documentation/io/built-in/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZDYehqpPpXY",
        "colab_type": "text"
      },
      "source": [
        "# [Windowing](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
        "\n",
        "As discussed above, windowing subdivides a `PCollection` according to the timestamps of its individual elements.\n",
        "\n",
        "Some Beam transforms, such as `GroupByKey` and `Combine`, group multiple elements by a common key. Ordinarily, that grouping operation groups all of the elements that have the same key within the entire data set. With an unbounded data set, it is impossible to collect all of the elements, since new elements are constantly being added and may be infinitely many (e.g. streaming data). If you are working with unbounded `PCollections`, windowing is especially useful.\n",
        "\n",
        "In the Beam model, any `PCollection` (including unbounded `PCollections`) can be subdivided into logical windows. Each element in a `PCollection` is assigned to one or more windows according to the `PCollection`'s windowing function, and each individual window contains a finite number of elements. Grouping transforms then consider each `PCollection`'s elements on a per-window basis. `GroupByKey`, for example, implicitly groups the elements of a `PCollection` by key and window.\n",
        "\n",
        "Additional information on Beam Windowing is available in the [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/#windowing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd3a4hwlVqfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DAYS = 24 * 60 * 60\n",
        "\n",
        "class AssignTimestamps(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    date = datetime.strptime(element[0], \"%Y-%m-%d\")\n",
        "    yield beam.window.TimestampedValue(element, date.timestamp())\n",
        "\n",
        "with beam.Pipeline() as window_pipeline:\n",
        "  lines = (window_pipeline\n",
        "           | beam.io.ReadFromText(_data_filepath)\n",
        "           | beam.Filter(lambda line: line < \"2014-01-01\")\n",
        "           | beam.Map(lambda line: line.split(\",\")) # CSV parser?\n",
        "           | beam.ParDo(AssignTimestamps())\n",
        "           | beam.WindowInto(beam.window.FixedWindows(7*DAYS))\n",
        "           | beam.Map(lambda cols: (cols[2], float(cols[-1])))\n",
        "           | beam.combiners.Mean.PerKey()\n",
        "           | beam.ParDo(print))\n",
        "\n",
        "display_pipeline(window_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQJxF3qPoaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AssignTimestamps(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    date = datetime.strptime(element[0], \"%Y-%m-%d\")\n",
        "    yield beam.window.TimestampedValue(element, date.timestamp())\n",
        "\n",
        "class PrintWithTimestamp(beam.DoFn):\n",
        "  def process(self, element, timestamp=beam.DoFn.TimestampParam):\n",
        "    print(timestamp.to_rfc3339()[:10], element)\n",
        "\n",
        "with beam.Pipeline() as ts_pipeline:\n",
        "  lines = (ts_pipeline\n",
        "           | beam.io.ReadFromText(_data_filepath)\n",
        "           | beam.Filter(lambda line: line < \"2014-01-01\")\n",
        "           | beam.Map(lambda line: line.split(\",\")) # CSV parser?\n",
        "           | beam.ParDo(AssignTimestamps())\n",
        "           | beam.WindowInto(beam.window.FixedWindows(7 * DAYS))\n",
        "           | beam.Map(lambda cols: (cols[2], float(cols[-1])))\n",
        "           | beam.combiners.Mean.PerKey()\n",
        "           | beam.ParDo(PrintWithTimestamp()))\n",
        "\n",
        "display_pipeline(ts_pipeline)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
